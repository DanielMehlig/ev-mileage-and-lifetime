{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime as dt\n",
    "\n",
    "print(os.getcwd)\n",
    "os.chdir(\"..\")\n",
    "script_path = os.getcwd()\n",
    "data_path = os.path.join(script_path, \"data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Before processing the MOT Data, read in the vehicle attribute databases and produce the RFR models for CO2 and mass\n",
    "\n",
    "# Read in databases\n",
    "# 1.1 BEV data for EL\n",
    "# from EV Database https://ev-database.org/uk/\n",
    "fpath = os.path.join(data_path, \"ev_data\", \"bev_spec.csv\")\n",
    "bev_spec = pd.read_csv(fpath)\n",
    "\n",
    "# only keep cols we need:\n",
    "bev_spec = bev_spec[['first_use_year', 'make', 'model', \"fuel efficiency Wh/mi\", \"battery capacity (kWh)\", \"mass (kg)\"]]\n",
    "# drop duplicates and keep first in index order\n",
    "bev_spec = bev_spec.drop_duplicates(keep='first')\n",
    "\n",
    "# 1.2 Read in the VCA data for PHEVs\n",
    "fpath = os.path.join(data_path, \"vca_data\", \"vca_phev_lookup.csv\")\n",
    "vca_phev_lookup = pd.read_csv(fpath)\n",
    "# Modify the vca_phev_lookup data to match the MOT data\n",
    "vca_phev_lookup[\"fuel_type\"] = vca_phev_lookup[\"Fuel Type\"]\n",
    "vca_phev_lookup.loc[vca_phev_lookup[\"fuel_type\"] == \"Petrol\", \"fuel_type\"] = \"PE\"\n",
    "vca_phev_lookup.loc[vca_phev_lookup[\"fuel_type\"] == \"Petrol Electric\", \"fuel_type\"] = \"HY\"\n",
    "vca_phev_lookup.loc[vca_phev_lookup[\"fuel_type\"] == \"Electricity / Petrol\", \"fuel_type\"] = \"HY\"\n",
    "vca_phev_lookup.loc[vca_phev_lookup[\"fuel_type\"] == \"Petrol Hybrid\", \"fuel_type\"] = \"HY\"      \n",
    "vca_phev_lookup.loc[vca_phev_lookup[\"fuel_type\"] == \"Electricity\", \"fuel_type\"] = \"EL\"\n",
    "vca_phev_lookup.loc[vca_phev_lookup[\"fuel_type\"] == \"Diesel\", \"fuel_type\"] = \"DI\"\n",
    "vca_phev_lookup.loc[vca_phev_lookup[\"fuel_type\"] == \"Diesel Electric\", \"fuel_type\"] = \"HY\"\n",
    "vca_phev_lookup.loc[vca_phev_lookup[\"fuel_type\"] == \"Diesel Hybrid\", \"fuel_type\"] = \"HY\"\n",
    "vca_phev_lookup.loc[vca_phev_lookup[\"fuel_type\"] == \"Electricity / Diesel\", \"fuel_type\"] = \"HY\"\n",
    "vca_phev_lookup = vca_phev_lookup.drop_duplicates()\n",
    "\n",
    "# 1.3 VCA Database RFR Model for CO2 - for DI PE HY\n",
    "# From VCA https://carfueldata.vehicle-certification-agency.gov.uk/\n",
    "data = []\n",
    "for year in range(2001, 2022, 1):\n",
    "    fname = os.path.join(data_path, \"vca_data\", f\"vca_{year}.csv\")\n",
    "    data_year = pd.read_csv(fname, quoting=1, encoding='ISO-8859-1')\n",
    "    data_year['Year'] = year\n",
    "    data.append(data_year)\n",
    "data = pd.concat(data, ignore_index=True)\n",
    "# Process the VCA data\n",
    "# Convert the numeric\n",
    "data[\"Engine Capacity\"] = pd.to_numeric(data[\"Engine Capacity\"], errors='coerce')\n",
    "data[\"CO2 g/km\"] = pd.to_numeric(data[\"CO2 g/km\"], errors='coerce')\n",
    "# Modify the data data to match the MOT data\n",
    "data[\"fuel_type\"] = data[\"Fuel Type\"]\n",
    "data.loc[data[\"fuel_type\"] == \"Petrol\", \"fuel_type\"] = \"PE\"\n",
    "data.loc[data[\"fuel_type\"] == \"Petrol Electric\", \"fuel_type\"] = \"HY\"\n",
    "data.loc[data[\"fuel_type\"] == \"Electricity / Petrol\", \"fuel_type\"] = \"HY\"\n",
    "data.loc[data[\"fuel_type\"] == \"Petrol Hybrid\", \"fuel_type\"] = \"HY\"      \n",
    "data.loc[data[\"fuel_type\"] == \"Electricity\", \"fuel_type\"] = \"EL\"\n",
    "data.loc[data[\"fuel_type\"] == \"Diesel\", \"fuel_type\"] = \"DI\"\n",
    "data.loc[data[\"fuel_type\"] == \"Diesel Electric\", \"fuel_type\"] = \"HY\"\n",
    "data.loc[data[\"fuel_type\"] == \"Diesel Hybrid\", \"fuel_type\"] = \"HY\"\n",
    "data.loc[data[\"fuel_type\"] == \"Electricity / Diesel\", \"fuel_type\"] = \"HY\"\n",
    "# remove EL\n",
    "data = data[data[\"fuel_type\"] != \"EL\"]\n",
    "data = data[data[\"fuel_type\"].isin([\"DI\", \"PE\", \"HY\"])]\n",
    "\n",
    "# Before calculating emissions, we need to adjust the CO2\n",
    "# emissions for NEDC & WLTP to be reaL world emissions\n",
    "# using the correction factors from the icct papers\n",
    "adjustment_factors = {\n",
    "    2000 : { \"DI\": 1.08, \"PE\": 1.04, \"HY\": 1.20,}, # NEDC\n",
    "    2001 : { \"DI\": 1.08, \"PE\": 1.04, \"HY\": 1.20,}, # NEDC\n",
    "    2002 : { \"DI\": 1.10, \"PE\": 1.05, \"HY\": 1.20,}, # NEDC\n",
    "    2003 : { \"DI\": 1.10, \"PE\": 1.06, \"HY\": 1.20,}, # NEDC\n",
    "    2004 : { \"DI\": 1.11, \"PE\": 1.07, \"HY\": 1.20,}, # NEDC\n",
    "    2005 : { \"DI\": 1.13, \"PE\": 1.09, \"HY\": 1.20,}, # NEDC\n",
    "    2006 : { \"DI\": 1.12, \"PE\": 1.10, \"HY\": 1.19,}, # NEDC\n",
    "    2007 : { \"DI\": 1.14, \"PE\": 1.12, \"HY\": 1.19,}, # NEDC\n",
    "    2008 : { \"DI\": 1.17, \"PE\": 1.14, \"HY\": 1.19,}, # NEDC\n",
    "    2009 : { \"DI\": 1.18, \"PE\": 1.17, \"HY\": 1.29,}, # NEDC\n",
    "    2010 : { \"DI\": 1.22, \"PE\": 1.19, \"HY\": 1.31,}, # NEDC\n",
    "    2011 : { \"DI\": 1.26, \"PE\": 1.22, \"HY\": 1.36,}, # NEDC\n",
    "    2012 : { \"DI\": 1.29, \"PE\": 1.26, \"HY\": 1.41,}, # NEDC\n",
    "    2013 : { \"DI\": 1.34, \"PE\": 1.30, \"HY\": 1.44,}, # NEDC\n",
    "    2014 : { \"DI\": 1.38, \"PE\": 1.32, \"HY\": 1.46,}, # NEDC\n",
    "    2015 : { \"DI\": 1.43, \"PE\": 1.35, \"HY\": 1.49,}, # NEDC\n",
    "    2016 : { \"DI\": 1.45, \"PE\": 1.36, \"HY\": 1.53,}, # NEDC\n",
    "    2017 : { \"DI\": 1.45, \"PE\": 1.37, \"HY\": 1.51,}, # NEDC\n",
    "    2018 : { \"DI\": 1.44, \"PE\": 1.37, \"HY\": 1.50,}, # NEDC\n",
    "    2019 : { \"DI\": 1.07, \"PE\": 1.09, \"HY\": 1.09,}, # WLTP note HEVs use petrol correction factor for WLTP (ICCT do not provide correction factors for HEVs)\n",
    "    2020 : { \"DI\": 1.10, \"PE\": 1.11, \"HY\": 1.11,}, # WLTP note HEVs use petrol correction factor for WLTP (ICCT do not provide correction factors for HEVs)\n",
    "    2021 : { \"DI\": 1.10, \"PE\": 1.11, \"HY\": 1.11,}, # WLTP note HEVs use petrol correction factor for WLTP (ICCT do not provide correction factors for HEVs)\n",
    "}\n",
    "for year in range(2001, 2022, 1):\n",
    "    for fuel in [\"DI\", \"PE\", \"HY\"]:\n",
    "        data.loc[(data[\"Year\"] == year) & (data[\"fuel_type\"] == fuel), \"CO2 g/km\"] = data.loc[(data[\"Year\"] == year) & (data[\"fuel_type\"] == fuel), \"CO2 g/km\"] * adjustment_factors[year][fuel]\n",
    "\n",
    "\n",
    "# find the CO2 per km for each year - just for reference\n",
    "for year in range(2001, 2022, 1):\n",
    "    data_year = data[data[\"Year\"] == year]\n",
    "    mean_year = round(data_year['CO2 g/km'].mean())\n",
    "    di_year = round(data_year.loc[data_year[\"fuel_type\"] == \"DI\", \"CO2 g/km\"].mean())\n",
    "    pe_year = round(data_year.loc[data_year[\"fuel_type\"] == \"PE\", \"CO2 g/km\"].mean())\n",
    "    try:\n",
    "        hy_year = round(data_year.loc[data_year[\"fuel_type\"] == \"HY\", \"CO2 g/km\"].mean())\n",
    "    except:\n",
    "        hy_year = 0\n",
    "    print(f\"Year: {year}, CO2 g/km: {mean_year}, DI: {di_year}, PE: {pe_year}, HY: {hy_year}\")\n",
    "\n",
    "# drop any rows with nans\n",
    "data = data[[\"fuel_type\", \"Year\", \"Engine Capacity\", \"CO2 g/km\",]].dropna()\n",
    "\n",
    "X = data[[\"fuel_type\", \"Year\", \"Engine Capacity\"]]\n",
    "y = data[\"CO2 g/km\"]\n",
    "\n",
    "# Data Preprocessing\n",
    "X = X.copy()\n",
    "y = y.copy()\n",
    "# Encode categorical variables\n",
    "le = LabelEncoder()\n",
    "X['fuel_type'] = le.fit_transform(X['fuel_type'])\n",
    "# Store feature names\n",
    "co2_feature_names = [\"Fuel Type\", \"Year\", \"Engine Capacity\"]\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"CO2 model data shape:\", X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# Create and train the model\n",
    "rf_model_co2 = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    min_samples_split=25,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model_co2.fit(X, y)\n",
    "rf_model_co2.fit(X_train, y_train)\n",
    "\n",
    "# Calculate and print R-squared for CO2 model\n",
    "co2_train_r2 = rf_model_co2.score(X_train, y_train)\n",
    "co2_test_r2 = rf_model_co2.score(X_test, y_test)\n",
    "print(f\"\\nCO2 Model R-squared - Training: {co2_train_r2:.4f}, Testing: {co2_test_r2:.4f}\")\n",
    "\n",
    "# Get and print feature importance for CO2 model\n",
    "co2_importance = rf_model_co2.feature_importances_\n",
    "print(\"\\nCO2 Model Feature Importance:\")\n",
    "for name, imp in zip(co2_feature_names, co2_importance):\n",
    "    print(f\"{name}: {imp:.4f}\")\n",
    "print(\"\\n\")    \n",
    "\n",
    "# Finally plot the two models predictons vs test\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# CO2 model\n",
    "y_pred = rf_model_co2.predict(X_test)\n",
    "ax1.hexbin(y_test, y_pred, gridsize=50, cmap='Blues', extent=(0, 350, 0, 350))\n",
    "ax1.set_xlim(0, 350)\n",
    "ax1.set_ylim(0, 350)\n",
    "ax1.set_ylabel(\"Predicted CO2 g/km\")\n",
    "ax1.set_xlabel(\"Actual CO2 g/km\")\n",
    "\n",
    "# 1.4 EEA Database RFR Model for Mass - for DI PE HY\n",
    "# From EEA - data is in multiple formats\n",
    "data = []\n",
    "for year in range(2010, 2022, 1):\n",
    "    fname = os.path.join(data_path, \"eea_data\", f\"eea_{year}.csv\")\n",
    "    if year < 2017:\n",
    "        data_year = pd.read_csv(fname, usecols=['MS', 'Ft', 'Ec (cm3)', 'Enedc (g/km)',  'M (kg)'], engine='pyarrow')\n",
    "        data_year['Ewltp (g/km)'] = np.nan\n",
    "        data_year['z (Wh/km)'] = np.nan\n",
    "    elif year == 2017:\n",
    "        data_year = pd.read_csv(fname, encoding=\"utf-16-le\", usecols=['MS', 'Ft', 'ec (cm3)', 'Enedc (g/km)', 'Ewltp (g/km)',  'm (kg)', 'z (Wh/km)'], sep='\\t', engine='pyarrow')\n",
    "        data_year = data_year.rename(columns={'ec (cm3)': 'Ec (cm3)', 'm (kg)': 'M (kg)'})\n",
    "    elif year == 2018:\n",
    "        data_year = pd.read_csv(fname, encoding=\"utf-8\", usecols=['MS', 'Ft', 'ec (cm3)', 'Enedc (g/km)', 'Ewltp (g/km)',  'm (kg)', 'z (Wh/km)'], sep='\\t', engine='pyarrow')\n",
    "        data_year = data_year.rename(columns={'ec (cm3)': 'Ec (cm3)', 'm (kg)': 'M (kg)'})\n",
    "    elif year >= 2019:\n",
    "        data_year = pd.read_csv(fname, encoding=\"utf-8\", usecols=['Country', 'Ft', 'ec (cm3)', 'Enedc (g/km)', 'Ewltp (g/km)',  'm (kg)', 'z (Wh/km)'], sep=',', engine='pyarrow')\n",
    "        data_year = data_year.rename(columns={'ec (cm3)': 'Ec (cm3)', 'm (kg)': 'M (kg)', 'Country': 'MS'})\n",
    "        \n",
    "    data_year['Year'] = year  \n",
    "    data_year = data_year.rename(columns={'Ec (cm3)': 'Engine Capacity', 'Enedc (g/km)': 'NEDC CO2 g/km', 'Ewltp (g/km)': 'WLTP CO2 g/km', 'M (kg)': 'mass (kg)', 'Ft': 'fuel_type'})\n",
    "    data_year = data_year.drop_duplicates()\n",
    "    data.append(data_year)\n",
    "data = pd.concat(data, ignore_index=True)\n",
    "\n",
    "# Process the EEA data\n",
    "# # Keep only NEDC up to 2018 and WLTP from 2019\n",
    "data[\"CO2 g/km\"] = data[\"NEDC CO2 g/km\"]\n",
    "data.loc[data[\"Year\"] >= 2019, \"CO2 g/km\"] = data.loc[data[\"Year\"] >= 2019, \"WLTP CO2 g/km\"]\n",
    "data = data.drop(columns=['NEDC CO2 g/km', 'WLTP CO2 g/km'])\n",
    "# Convert the numeric\n",
    "data[\"Engine Capacity\"] = pd.to_numeric(data[\"Engine Capacity\"], errors='coerce')\n",
    "data[\"CO2 g/km\"] = pd.to_numeric(data[\"CO2 g/km\"], errors='coerce')\n",
    "data[\"mass (kg)\"] = pd.to_numeric(data[\"mass (kg)\"], errors='coerce')\n",
    "data[\"Enedc (g/km)\"] = pd.to_numeric(data[\"CO2 g/km\"], errors='coerce')\n",
    "data[\"Ewltp (g/km)\"] = pd.to_numeric(data[\"CO2 g/km\"], errors='coerce')\n",
    "data[\"z (Wh/km)\"] = pd.to_numeric(data[\"z (Wh/km)\"], errors='coerce')\n",
    "# Modify the data data to match the MOT data\n",
    "data[\"fuel_type\"] = data[\"fuel_type\"]\n",
    "data.loc[data[\"fuel_type\"] == \"Petrol\", \"fuel_type\"] = \"PE\"\n",
    "data.loc[data[\"fuel_type\"] == \"PETROL\", \"fuel_type\"] = \"PE\"\n",
    "data.loc[data[\"fuel_type\"] == \"petrol\", \"fuel_type\"] = \"PE\"\n",
    "data.loc[data[\"fuel_type\"] == \"Petrol/Electric\", \"fuel_type\"] = \"HY\"\n",
    "data.loc[data[\"fuel_type\"] == \"PETROL/ELECTRIC\", \"fuel_type\"] = \"HY\"\n",
    "data.loc[data[\"fuel_type\"] == \"petrol/electric\", \"fuel_type\"] = \"HY\"\n",
    "data.loc[data[\"fuel_type\"] == \"petrol-electric\", \"fuel_type\"] = \"HY\"\n",
    "data.loc[data[\"fuel_type\"] == \"Petrol-Electric\", \"fuel_type\"] = \"HY\"\n",
    "data.loc[data[\"fuel_type\"] == \"PETROL-ELECTRIC\", \"fuel_type\"] = \"HY\"\n",
    "data.loc[data[\"fuel_type\"] == \"Diesel\", \"fuel_type\"] = \"DI\"\n",
    "data.loc[data[\"fuel_type\"] == \"DIESEL\", \"fuel_type\"] = \"DI\"\n",
    "data.loc[data[\"fuel_type\"] == \"diesel\", \"fuel_type\"] = \"DI\"\n",
    "data.loc[data[\"fuel_type\"] == \"Diesel/Electric\", \"fuel_type\"] = \"HY\"\n",
    "data.loc[data[\"fuel_type\"] == \"DIESEL/ELECTRIC\", \"fuel_type\"] = \"HY\"\n",
    "data.loc[data[\"fuel_type\"] == \"diesel/electric\", \"fuel_type\"] = \"HY\"\n",
    "data.loc[data[\"fuel_type\"] == \"diesel-electric\", \"fuel_type\"] = \"HY\"\n",
    "data.loc[data[\"fuel_type\"] == \"Diesel-Electric\", \"fuel_type\"] = \"HY\"\n",
    "data.loc[data[\"fuel_type\"] == \"DIESEL-ELECTRIC\", \"fuel_type\"] = \"HY\"\n",
    "data.loc[data[\"fuel_type\"] == \"Electric\", \"fuel_type\"] = \"EL\"\n",
    "data.loc[data[\"fuel_type\"] == \"ELECTRIC\", \"fuel_type\"] = \"EL\"\n",
    "data.loc[data[\"fuel_type\"] == \"electric\", \"fuel_type\"] = \"EL\"\n",
    "# only keep DI PE and HY\n",
    "data = data[data[\"fuel_type\"].isin([\"DI\", \"PE\", \"HY\"])]\n",
    "# Lastly, exclude any 'z (Wh/km)' values that are not 0 - excludes PHEVs\n",
    "# fill nan as 0\n",
    "data[\"z (Wh/km)\"] = data[\"z (Wh/km)\"].fillna(0)\n",
    "data = data[data[\"z (Wh/km)\"] == 0]\n",
    "print(f\"Length of data: {len(data)}\")\n",
    "data = data[['fuel_type', 'Year', 'Engine Capacity', 'CO2 g/km', 'mass (kg)']].dropna()\n",
    "data = data.drop_duplicates()\n",
    "print(f\"Length of data: {len(data)}\")\n",
    "\n",
    "# Adjuts type approval to real world using the ICCT adjustment factors\n",
    "for year in range(2010, 2022, 1):\n",
    "    for fuel in [\"DI\", \"PE\", \"HY\"]:\n",
    "        data.loc[(data[\"Year\"] == year) & (data[\"fuel_type\"] == fuel), \"CO2 g/km\"] = data.loc[(data[\"Year\"] == year) & (data[\"fuel_type\"] == fuel), \"CO2 g/km\"] * adjustment_factors[year][fuel]\n",
    "\n",
    "# find the mass for each year\n",
    "for year in range(2010, 2022, 1):\n",
    "    data_year = data[data[\"Year\"] == year]\n",
    "    mean_year = round(data_year[\"mass (kg)\"].mean())\n",
    "    di_year = round(data_year.loc[data_year[\"fuel_type\"] == \"DI\", \"mass (kg)\"].mean())\n",
    "    pe_year = round(data_year.loc[data_year[\"fuel_type\"] == \"PE\", \"mass (kg)\"].mean())\n",
    "    try:\n",
    "        hy_year = round(data_year.loc[data_year[\"fuel_type\"] == \"HY\", \"mass (kg)\"].mean())\n",
    "    except:\n",
    "        hy_year = 0\n",
    "    # print for refrence\n",
    "    print(f\"Year: {year}, mass (kg): {mean_year}, DI: {di_year}, PE: {pe_year}, HY: {hy_year}\")\n",
    "    \n",
    "# drop any rows with nans\n",
    "data = data[[\"fuel_type\", \"Year\", \"Engine Capacity\", \"CO2 g/km\", \"mass (kg)\"]].dropna()\n",
    "\n",
    "# Create RFR model to predict mass\n",
    "X = data[[\"fuel_type\", \"Year\", \"Engine Capacity\", \"CO2 g/km\"]]\n",
    "y = data[\"mass (kg)\"]\n",
    "# Data Preprocessing\n",
    "X = X.copy()\n",
    "y = y.copy()\n",
    "# Encode categorical variables\n",
    "le = LabelEncoder()\n",
    "X['fuel_type'] = le.fit_transform(X['fuel_type'])\n",
    "# Store feature names\n",
    "mass_feature_names = [\"Fuel Type\", \"Year\", \"Engine Capacity\", \"CO2 g/km\"]\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Mass model data shape:\", X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# Create and train the model\n",
    "rf_model_mass = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    min_samples_split=25,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model_mass.fit(X_train, y_train)\n",
    "\n",
    "# Calculate and print R-squared for mass model\n",
    "mass_train_r2 = rf_model_mass.score(X_train, y_train)\n",
    "mass_test_r2 = rf_model_mass.score(X_test, y_test)\n",
    "print(f\"\\nMass Model R-squared - Training: {mass_train_r2:.4f}, Testing: {mass_test_r2:.4f}\")\n",
    "\n",
    "# Get and print feature importance for mass model\n",
    "mass_importance = rf_model_mass.feature_importances_\n",
    "print(\"\\nMass Model Feature Importance:\")\n",
    "for name, imp in zip(mass_feature_names, mass_importance):\n",
    "    print(f\"{name}: {imp:.4f}\")\n",
    "print(\"\\n\")     \n",
    "\n",
    "# Mass model\n",
    "X_test_mass = X_test\n",
    "y_test_mass = y_test\n",
    "y_pred_mass = rf_model_mass.predict(X_test_mass)\n",
    "ax2.hexbin(y_test_mass, y_pred_mass, gridsize=50, cmap='Blues', extent=(0, 3000, 0, 3000))\n",
    "ax2.set_xlim(0, 3000)\n",
    "ax2.set_ylim(0, 3000)\n",
    "ax2.set_ylabel(\"Predicted mass (kg)\")\n",
    "ax2.set_xlabel(\"Actual mass (kg)\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Fianlly repeat predictions for CO2 using the eea data\n",
    "# drop any rows with nans\n",
    "data = data[[\"fuel_type\", \"Year\", \"Engine Capacity\", \"CO2 g/km\"]].dropna()\n",
    "# Create RFR model to predict mass\n",
    "X = data[[\"fuel_type\", \"Year\", \"Engine Capacity\",]]\n",
    "y = data[ \"CO2 g/km\"]\n",
    "# Data Preprocessing\n",
    "X = X.copy()\n",
    "y = y.copy()\n",
    "# Encode categorical variables\n",
    "le = LabelEncoder()\n",
    "X['fuel_type'] = le.fit_transform(X['fuel_type'])\n",
    "# Store feature names\n",
    "co2_feature_names = [\"Fuel Type\", \"Year\", \"Engine Capacity\"]\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"CO2 EEA model data shape\", X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# Create and train the model\n",
    "rf_model_co2_eea = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    min_samples_split=25,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model_co2_eea.fit(X_train, y_train)\n",
    "\n",
    "# Calculate and print R-squared for mass model\n",
    "mass_train_r2 = rf_model_co2_eea.score(X_train, y_train)\n",
    "mass_test_r2 = rf_model_co2_eea.score(X_test, y_test)\n",
    "print(f\"\\nMass Model R-squared - Training: {mass_train_r2:.4f}, Testing: {mass_test_r2:.4f}\")\n",
    "\n",
    "# Get and print feature importance for mass model\n",
    "co2_eea_importance = rf_model_co2_eea.feature_importances_\n",
    "print(\"\\nMass Model Feature Importance:\")\n",
    "for name, imp in zip(co2_feature_names, co2_eea_importance):\n",
    "    print(f\"{name}: {imp:.4f}\")\n",
    "print(\"\\n\")   \n",
    "\n",
    "# Finally plot the two models predictons vs test\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# CO2 model\n",
    "y_pred = rf_model_co2_eea.predict(X_test)\n",
    "ax1.hexbin(y_test, y_pred, gridsize=50, cmap='Blues', extent=(0, 350, 0, 350))\n",
    "ax1.set_xlim(0, 350)\n",
    "ax1.set_ylim(0, 350)\n",
    "ax1.set_ylabel(\"Predicted CO2 g/km\")\n",
    "ax1.set_xlabel(\"Actual CO2 g/km\")\n",
    "  \n",
    "# Mass model\n",
    "# y_pred_mass = rf_model_mass.predict(X_test_mass)\n",
    "ax2.hexbin(y_test_mass, y_pred_mass, gridsize=50, cmap='Blues', extent=(0, 3000, 0, 3000))\n",
    "ax2.set_xlim(0, 3000)\n",
    "ax2.set_ylim(0, 3000)\n",
    "ax2.set_ylabel(\"Predicted mass (kg)\")\n",
    "ax2.set_xlabel(\"Actual mass (kg)\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Now pre-process the MOT data - add new cols for CO2 per km, mass, battery size, fuel efficiency, age, and age rounded  \n",
    "# Loop over MOT data by year\n",
    "# MOT data from https://www.data.gov.uk/dataset/e3939ef8-30c7-4ca8-9c7c-ad9475cc9b2f/anonymised_mot_test\n",
    "for year in tqdm(list(range(2023, 2004, -1)) ):\n",
    "    print(year)\n",
    "\n",
    "    # Columns to read from the MOT reuslts data\n",
    "    results_columns = [\n",
    "        \"vehicle_id\",\n",
    "        \"test_date\",\n",
    "        \"test_class_id\",\n",
    "        \"test_result\",\n",
    "        \"test_mileage\",\n",
    "        \"make\",\n",
    "        \"model\",\n",
    "        \"fuel_type\",\n",
    "        \"cylinder_capacity\",\n",
    "        \"first_use_date\",\n",
    "        ]\n",
    "\n",
    "    # Get MOT 'Results' \n",
    "    directory = str(year) + \"_Result\"\n",
    "    filename = f\"{year}_all_results.csv\"\n",
    "    file_path =  os.path.join(data_path, directory, filename)\n",
    "    print(f\"Input: {filename}\")\n",
    "    df = pd.read_csv(file_path, engine='pyarrow', dtype_backend='pyarrow', usecols=results_columns)\n",
    "    \n",
    "    # print number of unique vehicles for each fuel type\n",
    "    for fuel in [\"EL\", \"HY\", \"PE\", \"DI\"]:\n",
    "        n_vehicles = df.loc[df[\"fuel_type\"] == fuel, \"vehicle_id\"].nunique()\n",
    "        print(f\"In {year}, number of unique {fuel} vehicles: {n_vehicles}\")\n",
    "        \n",
    "    print(\"Preprocessing the MOT data\")\n",
    "    \n",
    "    print(\"\\tCylinder capacity\")\n",
    "    # convert cylinder_capacity to numeric\n",
    "    df[\"cylinder_capacity\"] = pd.to_numeric(df[\"cylinder_capacity\"], errors='coerce')\n",
    "    df[\"cylinder_capacity\"] = df[\"cylinder_capacity\"].fillna(0) # catch nans for BEVs\n",
    "    df[\"cylinder_capacity_rounded\"] = df[\"cylinder_capacity\"].apply(lambda x: round(x, -2)) # used for VCA data matching PHEVs\n",
    "    # change any EL cylinder_capacity > 0 to HY\n",
    "    df.loc[(df[\"cylinder_capacity\"] > 0) & (df[\"fuel_type\"]==\"EL\"), \"fuel_type\"] = \"HY\"\n",
    "    # Now rename other fuel types to be consistent\n",
    "    \n",
    "    print(\"\\tFuel Type Labels\")\n",
    "    df.loc[df[\"fuel_type\"] == \"Electric\", \"fuel_type\"] = \"EL\"\n",
    "    df.loc[df[\"fuel_type\"] == \"Hybrid Electric (Clean)\", \"fuel_type\"] = \"HY\"\n",
    "    df.loc[df[\"fuel_type\"] == \"ED\", \"fuel_type\"] = \"HY\" # Electric Diesel Hybrids\n",
    "    # Then only keep the fuels we are interested in (excluding Diesel Hyrbids)\n",
    "    df = df[df[\"fuel_type\"].isin([\"EL\", \"HY\", \"PE\", \"DI\"])]\n",
    "    \n",
    "    # Convert dates in datetime objects & get age\n",
    "    print(\"\\tDatetime columns\")\n",
    "    df['first_use_date'] = pd.to_datetime(df['first_use_date'], format=\"%Y-%m-%d\", errors='coerce')\n",
    "    # Cap first use date to 1980 (25 years before the earliest MOT data)\n",
    "    df.loc[df['first_use_date'].dt.year < 1980, 'first_use_date'] = pd.to_datetime(\"1980-01-01\", format=\"%Y-%m-%d\", errors='coerce')\n",
    "    df['test_date'] = pd.to_datetime(df['test_date'], format=\"%Y-%m-%d\", errors='coerce')\n",
    "    df['first_use_year'] = df['first_use_date'].dt.year\n",
    "    df['test_year'] = df['test_date'].dt.year\n",
    "    # Drop rows with missing dates\n",
    "    df = df[df['first_use_date'].notna()]\n",
    "    df = df[df['test_date'].notna()]\n",
    "    # Calc age in years\n",
    "    df['age_day'] = df['test_date'] - df['first_use_date']\n",
    "    df['age_year'] = df['age_day'].dt.days / 365.25\n",
    "    df['age_year_rounded'] = round(df['age_year'])\n",
    "    \n",
    "    # Next match the MOT to the vehicle databases\n",
    "    print(\"Matching the MOT data to the vehicle databases\")\n",
    "    # Matching is seperate for EL vs DI PE HY \n",
    "    # And set up new col for CO2 (mass col is added with the merge with bev spec)\n",
    "    df[\"CO2 g/km\"] = 0.0\n",
    "\n",
    "    # Merge the BEV data\n",
    "    df = df.merge(bev_spec, on=[\"first_use_year\", \"make\", \"model\"], how=\"left\")\n",
    "    \n",
    "    # Next apply the random forest model to the data\n",
    "    print(\"Applying the RFR CO2 model to the data\")\n",
    "    \n",
    "    # Get mask of EL and HY\n",
    "    mask_EL = df[\"fuel_type\"] == \"EL\"  \n",
    "    mask_HY = df[\"fuel_type\"] == \"HY\"\n",
    "    \n",
    "    # Apply the RFR model for CO2 g/km \n",
    "    # first drop any rows with missing values for prediction\n",
    "    # Drop rows with missing values for the specified columns\n",
    "    cols = [\"fuel_type\", \"first_use_year\", \"cylinder_capacity\"]\n",
    "    df = df.dropna(subset=cols)\n",
    "    # Get the data in the correct format\n",
    "    X = df.loc[~mask_EL].copy()\n",
    "    X = X[cols]\n",
    "    #   Replace firt use years below 2001 with 2001\n",
    "    X.loc[X[\"first_use_year\"] < 2001, \"first_use_year\"] = 2001\n",
    "    X['fuel_type'] = le.transform(X['fuel_type'])\n",
    "    # rename columns to match training data\n",
    "    X.columns = [\"fuel_type\", \"Year\", \"Engine Capacity\"]\n",
    "    y = rf_model_co2.predict(X)\n",
    "    # Add the predicted CO2 values to the df\n",
    "    df.loc[~mask_EL, \"CO2 g/km\"] = y\n",
    "    \n",
    "    # Apply the RFR model for mass\n",
    "    print(\"Applying the RFR mass model to the data\")\n",
    "    # first drop any rows with missing values for prediction\n",
    "    cols = [\"fuel_type\", \"first_use_year\", \"cylinder_capacity\", \"CO2 g/km\"]\n",
    "    df = df.dropna(subset=cols)\n",
    "    # Get the data in the correct format\n",
    "    X = df.loc[~mask_EL].copy()\n",
    "    X = X[[\"fuel_type\", \"first_use_year\", \"cylinder_capacity\", \"CO2 g/km\"]]\n",
    "    #   Replace firt use years below 2010 with 2010\n",
    "    X.loc[X[\"first_use_year\"] < 2010, \"first_use_year\"] = 2010\n",
    "    X['fuel_type'] = le.transform(X['fuel_type'])\n",
    "    # rename columns to match training data\n",
    "    X.columns = [\"fuel_type\", \"Year\", \"Engine Capacity\", \"CO2 g/km\"]\n",
    "    y = rf_model_mass.predict(X)\n",
    "    # Add the predicted mass values to the df\n",
    "    df.loc[~mask_EL, \"mass (kg)\"] = y\n",
    "    \n",
    "    # Finally merge the vca data to remove PHEVs from the data\n",
    "    # This requiires time intesnive processing of the MOT data make and model cols to match the VCA data\n",
    "    print(\"Prep MOT data for VCA merge for PHEV identification\")\n",
    "    # Models are given first, then followed by trim descriptions\n",
    "    # Split the strings after the main model name - eg \"GOLF GTI\" -> \"GOLF\"\n",
    "    # Split the df up to make this much faster\n",
    "    df_HY = df.loc[mask_HY].copy()\n",
    "    df = df.loc[~mask_HY].copy() \n",
    "    # Exhaustive list of model names to remove detail not included in vca data - not necessary for all PHEVs but keep for now\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"GT\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"CRDI\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"CDI\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"TDI\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"TSI\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"TFSI\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"TDCI\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"R-DYN\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"HEV\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"MHEV\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"PHEV\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"HYBRID\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\" PLUS\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"+\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"RANGE EXTENDER\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"EXCEL\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"DESIGN\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"TOURING SPORTS\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"ICON\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"LIME EDITION \", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"ORANGE EDITION \", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"BLACK EDITION \", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"LAUNCH EDITION \", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"GR SPORT\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"TREK\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"ESTATE\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"INVINCIBLE\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"PRO AUTO\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"CITY\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"S-A\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"PREMIUM\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"LUXURY \", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\" SE \", n=1).str[0] # include spaces to avoid removing correct model names\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"FIRST EDITION\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"ULTIMATE\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"ST-LINE\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"TITANIUM\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"VIGNALE\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"ACTIVE\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"TREND\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"MATCH\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"FLYING\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"SPEED\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"CARBON\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"VORSPRUNG\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"TECHNIK\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"S LINE\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"S-LN\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"S LN\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"SLN\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"CABRIOLET\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"CITYCARVER\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"AUTO\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"MOMENTUM\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"INSCRIPTION\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"R-DESIGN\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"AWD\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"4WD\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"M SPORT\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"XDRIVE\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"AIRCORSS\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"CBACK\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"CROSSBACK\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"C-BACK\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"CROSSTAR\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\" EX \", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\" SR \", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\",\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"R-DYNAMIC\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"S P300E\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"HSE P300E\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\"EXECUTIVE\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\" 110 \", n=1).str[0] # MERC numbers:\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\" 114 \", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\" 116 \", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\" 160 \", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\" 180 \", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\" 200 \", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\" 250 \", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\" 300 \", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\" 450 \", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\" VISTA \", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\" R-LINE  \", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.split(\" SEL\", n=1).str[0]\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.replace(\"90 R-\", \"90\")\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.replace(\"90 R\", \"90\")\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.replace(\"60 R-\", \"90\")\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.replace(\"60 R\", \"90\")\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.replace(\"40 R-\", \"90\")\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.replace(\"40 R\", \"90\")\n",
    "    # Specific to Land Rover - inconsistent naming in the data for a large number of vehicles\n",
    "    df_HY.loc[df_HY[\"model\"].str.contains(\"DISCOVERY SPRT\", na=False), \"model\"] = \"DISCOVERY SPORT\"\n",
    "    df_HY.loc[df_HY[\"model\"].str.contains(\"DISCOVRY SPRT\", na=False), \"model\"] = \"DISCOVERY SPORT\"\n",
    "    df_HY.loc[df_HY[\"model\"].str.contains(\"R ROVER EVOQUE\", na=False), \"model\"] = \"RANGE ROVER EVOQUE\"\n",
    "    df_HY.loc[df_HY[\"model\"].str.contains(\"R ROVER SPORT\", na=False), \"model\"] = \"RANGE ROVER SPORT\"\n",
    "    df_HY.loc[df_HY[\"model\"].str.contains(\"R ROVER SPRT\", na=False), \"model\"] = \"RANGE ROVER SPORT\"\n",
    "    # Likewise for MERCEDES - inconsistent naming for A-Class HEVs (20,000 HEVs)\n",
    "    df_HY.loc[(df_HY[\"fuel_type\"] == \"car_hev\") & (df_HY[\"cylinder_capacity\"]== 1332), \"model\"] = \"A-CLASS\"\n",
    "    # Also change all A to A-CLASS, B to B-CLASS, etc\n",
    "    df_HY.loc[(df_HY[\"make\"] == \"MERCEDES-BENZ\") & (df_HY[\"model\"] == \"A\"), \"model\"] = \"A-CLASS\"\n",
    "    df_HY.loc[(df_HY[\"make\"] == \"MERCEDES-BENZ\") & (df_HY[\"model\"] == \"B\"), \"model\"] = \"B-CLASS\"\n",
    "    df_HY.loc[(df_HY[\"make\"] == \"MERCEDES-BENZ\") & (df_HY[\"model\"] == \"C\"), \"model\"] = \"C-CLASS\"\n",
    "    df_HY.loc[(df_HY[\"make\"] == \"MERCEDES-BENZ\") & (df_HY[\"model\"] == \"E\"), \"model\"] = \"E-CLASS\"\n",
    "    df_HY.loc[(df_HY[\"make\"] == \"MERCEDES-BENZ\") & (df_HY[\"model\"] == \"S\"), \"model\"] = \"S-CLASS\"\n",
    "    # Change MINI COOPER to MINI MINI\n",
    "    df_HY.loc[(df_HY[\"make\"] == \"MINI\") & (df_HY[\"model\"] == \"COOPER\"), \"model\"] = \"MINI\"\n",
    "    # For BMW, split at M and take the first part\n",
    "    df_HY.loc[(df_HY[\"make\"] == \"BMW\"), \"model\"] = df_HY.loc[(df_HY[\"make\"] == \"BMW\"), \"model\"].str.split(\"M\", n=1).str[0]\n",
    "    # Not for land rover or KIA, split at sport and take the first part \n",
    "    # avoiding: \"RANGE ROVER SPORT\" -> \"RANGE ROVER\", KIA \"SPORTAGE\" -> \"SPORTAGE\"\n",
    "    df_HY.loc[(df_HY[\"make\"] != \"LAND ROVER\") & (df_HY[\"make\"] != \"KIA\"), \"model\"] = df_HY.loc[(df_HY[\"make\"] != \"LAND ROVER\") & (df_HY[\"make\"] != \"KIA\"), \"model\"].str.split(\"SPORT\", n=1).str[0]\n",
    "    # Finally remove any trailing or leading whitespace\n",
    "    df_HY[\"model\"] = df_HY[\"model\"].str.strip()\n",
    "    df_HY[\"make\"] = df_HY[\"make\"].str.strip()\n",
    "    # Add the data back together\n",
    "    df = pd.concat([df, df_HY], ignore_index=True)\n",
    "    \n",
    "    # merge the data with the PHEV lookup\n",
    "    df = df.merge(vca_phev_lookup, \n",
    "             how='left', \n",
    "             left_on=['make', 'model', 'first_use_year', 'cylinder_capacity_rounded', 'fuel_type'], \n",
    "             right_on=['Manufacturer', 'Model', 'Year', 'Engine Capacity', 'fuel_type']\n",
    "             )\n",
    "    \n",
    "    # print number of unique vehicles for each fuel type\n",
    "    for fuel in df[\"fuel_type\"].unique():\n",
    "        n_vehicles = df.loc[df[\"fuel_type\"] == fuel, \"vehicle_id\"].nunique()\n",
    "        print(f\"After pre-processing {year}, number of unique {fuel} vehicles: {n_vehicles}\")\n",
    "        if fuel == \"HY\":\n",
    "            n_phevs = df.loc[(df[\"fuel_type\"] == fuel) & (df[\"ev_type\"] ==\"phev\"), \"vehicle_id\"].nunique()\n",
    "            print(f\"Number of PHEVs: {n_phevs} (2015-2020 Models)\")\n",
    "    \n",
    "    # Convert to parquet - speeds up later steps where only subsets (cols) of the data are needed\n",
    "    parquet_filename = f\"{year}_all_results.parquet\"\n",
    "    parquet_file_path = os.path.join(data_path, directory, parquet_filename)\n",
    "    df.to_parquet(parquet_file_path, engine='pyarrow', compression='snappy')\n",
    "    print(f\"Output: {parquet_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create Vehicle Sample using vehicle_id\n",
    "# The cohort is defined as vehicles that are 3 years old in the given year\n",
    "# I.e., the first tests for these vehicles when they enter the MOT data\n",
    "\n",
    "all_ids = {\n",
    "    \"lgv\": [], # note not used in the paper \n",
    "    \"car_diesel\": [],\n",
    "    \"car_petrol\": [],\n",
    "    \"car_hev\": [],\n",
    "    \"car_phev\": [],\n",
    "    \"car_bev\": [],\n",
    "    \"mcycle\" : [], # note not used in the paper \n",
    "}\n",
    "\n",
    "for year in tqdm(list(range(2023, 2004, -1)) ):\n",
    "    print(year)\n",
    "    directory = str(year) + \"_Result\"\n",
    "    filename = f\"{year}_all_results.parquet\"\n",
    "    file_path =  os.path.join(data_path, directory, filename)\n",
    "    print(f\"Input: {filename}\")\n",
    "    # Only read in minimum number of cols for faster processing of IDs\n",
    "    columns = ['vehicle_id', 'age_year_rounded', 'test_class_id', 'make', 'model', 'fuel_type', 'ev_type', 'battery capacity (kWh)', 'cylinder_capacity']\n",
    "    df = pd.read_parquet(file_path, engine='pyarrow', columns=columns)\n",
    "    \n",
    "    # Filter for 3 year old vehicles - this is the cohort for the test year\n",
    "    if year in [2005, 2006]:\n",
    "        df = df[(df['age_year_rounded'] >= 3) & (df['age_year_rounded'] <= 35)]\n",
    "        sample_size = 1_000_000 # larger sample size as ages 3-30 are included\n",
    "        # Note that this sample size is approx the same as the total number of vehicles in the MOT data for 2005 thus include 2006 as well\n",
    "    else:\n",
    "        df = df[df['age_year_rounded'] == 3]  \n",
    "        sample_size = 200_000 # only age 3 vehicles are included # only used for scrappage and mileage figures = not in training \n",
    "\n",
    "    # Get LGV IDs\n",
    "    print(\"\\tLGVs\")\n",
    "\n",
    "    # Filter df for LGVs predefined makes and models\n",
    "    # Remommneded by DfT since some LGVs are in class 4 and rest are in class 7\n",
    "    df_sample = df[\n",
    "        (\n",
    "            \n",
    "        # LGVs in class 4\n",
    "        (df['test_class_id'] == 4) & \n",
    "        (  ((df['make'].str.contains('FORD')) &\n",
    "            (df['model'].str.contains(\"TRANSIT\"))) |\n",
    "\n",
    "            ((df['make'].str.contains('MERCEDES')) &\n",
    "            (df['model'].str.contains(\"SPRINTER\"))) |\n",
    "\n",
    "            ((df['make'].str.contains('MERCEDES')) &\n",
    "            (df['model'].str.contains(\"VITO\"))) |\n",
    "\n",
    "            ((df['make'].str.contains('VAUXHALL')) &\n",
    "            (df['model'].str.contains(\"VIVARO\"))) |\n",
    "\n",
    "            ((df['make'].str.contains('VAUXHALL')) &\n",
    "            (df['model'].str.contains(\"COMBO\"))) |\n",
    "\n",
    "            ((df['make'].str.contains('VAUXHALL')) &\n",
    "            (df['model'].str.contains(\"MOVANO\"))) |\n",
    "\n",
    "            ((df['make'].str.contains('VOLKS')) &\n",
    "            (df['model'].str.contains(\"TRANSPORTER\"))) |\n",
    "\n",
    "            ((df['make'].str.contains('VOLKS')) &\n",
    "            (df['model'].str.contains(\"CADDY\"))) |\n",
    "\n",
    "            ((df['make'].str.contains('CITROEN')) &\n",
    "            (df['model'].str.contains(\"RELAY\"))) |\n",
    "\n",
    "            ((df['make'].str.contains('CITROEN')) &\n",
    "            (df['model'].str.contains(\"DISPATCH\"))) |\n",
    "\n",
    "            ((df['make'].str.contains('CITROEN')) &\n",
    "            (df['model'].str.contains(\"RELAY\"))) |\n",
    "\n",
    "            ((df['make'].str.contains('CITROEN')) &\n",
    "            (df['model'].str.contains(\"Berlingo\"))) |\n",
    "\n",
    "            ((df['make'].str.contains('PEUGEOT')) &\n",
    "            (df['model'].str.contains(\"PARTNER\"))) |\n",
    "\n",
    "            ((df['make'].str.contains('PEUGEOT')) &\n",
    "            (df['model'].str.contains(\"BOXER\"))) |\n",
    "\n",
    "            ((df['make'].str.contains('RENAULT')) &\n",
    "            (df['model'].str.contains(\"TRAFIC\"))) |\n",
    "\n",
    "            ((df['make'].str.contains('RENAULT')) &\n",
    "            (df['model'].str.contains(\"KANGOO\"))) |\n",
    "\n",
    "            ((df['make'].str.contains('FIAT')) &\n",
    "            (df['model'].str.contains(\"DUCATO\")))\n",
    "        )\n",
    "        \n",
    "        # LGVs in class 7 \n",
    "        ) |\n",
    "        (df['test_class_id'] == 7) \n",
    "        ]\n",
    "    \n",
    "    print(f\"\\t{year} Total LGVs: {df_sample['vehicle_id'].nunique()}\")\n",
    "    \n",
    "    all_ids[\"lgv\"].extend(df_sample['vehicle_id'].unique())\n",
    "\n",
    "    # Now get ID sample from filtered DF\n",
    "    sample_lgv = df_sample['vehicle_id'].unique()\n",
    "    sample_lgv = pd.DataFrame(sample_lgv, columns=['vehicle_id'])\n",
    "    \n",
    "    # only keep sample_size LGVs\n",
    "    if len(sample_lgv) > sample_size:\n",
    "        sample_lgv = sample_lgv.sample(n=sample_size, random_state=42)\n",
    "\n",
    "    directory = \"Sample_Data\" \n",
    "    filename = f\"sample_IDs_lgv_{year}.parquet\"\n",
    "    file_path =  os.path.join(data_path, directory, filename)\n",
    "    sample_lgv.to_parquet(file_path, engine='pyarrow', compression='snappy') \n",
    "    print(f\"\\tLGV Sample: {len(sample_lgv)}\")\n",
    "\n",
    "    # Remove LGVs from vehicle class 4\n",
    "    # Now only cars (mostly) remain in class 4 - see MOT docs\n",
    "    df = df [~df['vehicle_id'].isin(sample_lgv['vehicle_id'])]  \n",
    "\n",
    "    # Next Cars\n",
    "    # Diesel Cars first \n",
    "    print(\"\\tDiesel Cars\")\n",
    "    df_sample = df[(df['test_class_id'] == 4) & (df['fuel_type'] == 'DI')]\n",
    "    print(f\"\\t{year} Total DI: {df_sample['vehicle_id'].nunique()}\")\n",
    "    all_ids[\"car_diesel\"].extend(df_sample['vehicle_id'].unique())\n",
    "\n",
    "    # Now get ID sample from filtered DF\n",
    "    sample_di_car = df_sample['vehicle_id'].unique()\n",
    "    sample_di_car = pd.DataFrame(sample_di_car, columns=['vehicle_id'])\n",
    "    \n",
    "    # only keep sample_size DE\n",
    "    if len(sample_di_car) > sample_size:\n",
    "        sample_di_car = sample_di_car.sample(n=sample_size, random_state=42)\n",
    "            \n",
    "    directory = \"Sample_Data\"\n",
    "    filename = f\"sample_IDs_car_diesel_{year}.parquet\"\n",
    "    file_path =  os.path.join(data_path, directory, filename)\n",
    "    sample_di_car.to_parquet(file_path, engine='pyarrow', compression='snappy') \n",
    "    print(f\"\\tDiesel Car Sample: {len(sample_di_car)}\")\n",
    "\n",
    "    # Next Petrol Cars\n",
    "    print(\"\\tPetrol Cars\")\n",
    "    df_sample = df[(df['test_class_id'] == 4) &(df['fuel_type'] == 'PE') ]\n",
    "    print(f\"\\t{year} Total PE: {df_sample['vehicle_id'].nunique()}\")\n",
    "    all_ids[\"car_petrol\"].extend(df_sample['vehicle_id'].unique())\n",
    "\n",
    "    # Now get ID sample from filtered DF\n",
    "    sample_pe_car = df_sample['vehicle_id'].unique()\n",
    "    sample_pe_car = pd.DataFrame(sample_pe_car, columns=['vehicle_id'])\n",
    "    \n",
    "    # only keep sample_size DE\n",
    "    if len(sample_pe_car) > sample_size:\n",
    "        sample_pe_car = sample_pe_car.sample(n=sample_size, random_state=42)\n",
    "        \n",
    "    directory = \"Sample_Data\"\n",
    "    filename = f\"sample_IDs_car_petrol_{year}.parquet\"\n",
    "    file_path =  os.path.join(data_path, directory, filename)\n",
    "    sample_pe_car.to_parquet(file_path, engine='pyarrow', compression='snappy') \n",
    "    print(f\"\\tPetrol Car Sample: {len(sample_pe_car)}\")\n",
    "\n",
    "    # Next HEVs -  Sample is the entire HEV fleet - excluding PHEVs\n",
    "    print(\"\\tHEV Cars\")\n",
    "    df_phevs_sample = df[(df['test_class_id'] == 4) & (df['fuel_type'] == 'HY') & (df['ev_type'] == 'phev')]\n",
    "    df_sample = df[(df['test_class_id'] == 4) & (df['fuel_type'] == 'HY') & (df['ev_type'] != 'phev')]\n",
    "    print(f\"\\t{year} Total HY: {df_sample['vehicle_id'].nunique()}\")\n",
    "    print(f\"\\t{year} Total PHEVs exlcuded: {df_phevs_sample['vehicle_id'].nunique()}\")\n",
    "    all_ids[\"car_hev\"].extend(df_sample['vehicle_id'].unique())\n",
    "    all_ids['car_phev'].extend(df[(df['test_class_id'] == 4) & (df['fuel_type'] == 'HY') & (df['ev_type'] == 'phev')]['vehicle_id'].unique())\n",
    "    \n",
    "    # Now get ID sample from filtered DF - sample is all HEVS\n",
    "    sample_hy_car = df_sample[['vehicle_id']]\n",
    "    sample_hy_car = sample_hy_car.drop_duplicates()\n",
    "    directory = \"Sample_Data\"\n",
    "    filename = f\"sample_IDs_car_hev_{year}.parquet\"\n",
    "    file_path =  os.path.join(data_path, directory, filename)\n",
    "    sample_hy_car.to_parquet(file_path, engine='pyarrow', compression='snappy') \n",
    "    print(f\"\\tHEV Car Sample: {len(sample_hy_car)}\")\n",
    "\n",
    "    # BEV Cars -  Sample is the entire BEV fleet\n",
    "    print(\"\\tBEV Cars\")\n",
    "    df_sample = df[(df['test_class_id'] == 4)&(df['fuel_type'] == 'EL')]\n",
    "    print(f\"\\t{year} Total EL: {df_sample['vehicle_id'].nunique()}\")\n",
    "    \n",
    "    # Only keep vehicles if they have Battery Size data\n",
    "    df_sample = df_sample[~df_sample['battery capacity (kWh)'].isna()]\n",
    "    \n",
    "    # Now get sample from filtered DF\n",
    "    sample_el_car = df_sample[['vehicle_id']]\n",
    "    sample_el_car = sample_el_car.drop_duplicates()\n",
    "    directory = \"Sample_Data\"\n",
    "    filename = f\"sample_IDs_car_bev_{year}.parquet\"\n",
    "    file_path =  os.path.join(data_path, directory, filename)\n",
    "    sample_el_car.to_parquet(file_path, engine='pyarrow', compression='snappy') \n",
    "    print(f\"\\tBEV Car Sample: {len(sample_el_car)}\")\n",
    "    all_ids[\"car_bev\"].extend(df[(df['test_class_id'] == 4) & (df['fuel_type'] == 'EL')]['vehicle_id'].unique())\n",
    "    \n",
    "    # for refrence output bevs that are not in the sample\n",
    "    directory = \"Sample_Data\"\n",
    "    fname = f\"BEV_not_in_sample_{year}.csv\"\n",
    "    fpath = os.path.join(data_path, directory, fname)\n",
    "    df[df[\"vehicle_id\"].isin(all_ids[\"car_bev\"]) == False].to_csv(fpath, index=False)\n",
    "    \n",
    "    # Motorcycles \n",
    "    print(\"\\tMcycles\")\n",
    "\n",
    "    df_sample = df[(df['test_class_id'] == 1) | (df['test_class_id'] == 2)]\n",
    "    print(f\"\\t{year} Total MCycle: {df_sample['vehicle_id'].nunique()}\")\n",
    "    # Now get ID sample from filtered DF\n",
    "    sample_mcycle = df_sample['vehicle_id'].unique()  \n",
    "    sample_mcycle = pd.DataFrame(sample_mcycle, columns=['vehicle_id'])\n",
    "    \n",
    "    # only keep sample_size Motorcycles\n",
    "    if len(sample_mcycle) > sample_size:\n",
    "        sample_mcycle = sample_mcycle.sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    directory = \"Sample_Data\"\n",
    "    filename = f\"sample_IDs_mcycle_{year}.parquet\"\n",
    "    file_path =  os.path.join(data_path, directory, filename)\n",
    "    sample_mcycle.to_parquet(file_path, engine='pyarrow', compression='snappy') \n",
    "    print(f\"\\tMotorcycle Sample: {len(sample_mcycle)}\")\n",
    "    all_ids[\"mcycle\"].extend(df_sample['vehicle_id'].unique())\n",
    "    \n",
    "\n",
    "    # Clear memory - reduces RAM for intial input of next year's dfs\n",
    "    del [\n",
    "        df, sample_lgv, sample_di_car, sample_pe_car, \n",
    "        sample_hy_car, sample_el_car, sample_mcycle]\n",
    "    gc.collect()\n",
    "\n",
    "for fuel in all_ids.keys():\n",
    "    # Make sure there are no duplicates\n",
    "    all_ids[fuel] = list(set(all_ids[fuel]))\n",
    "    print(f\"{fuel}: {len(all_ids[fuel])}\")\n",
    "\n",
    "# Open MOT data for each year and save the data for the sample IDs\n",
    "# This reduces the size of the data to be processed in the next steps\n",
    "# And acts as a first set of filtering as we want only vehicles with their first test at 3 years old\n",
    "\n",
    "# IDs for LGVs, Diesel Cars, Petrol Cars, HEV Cars, BEV Cars, Motorcycles\n",
    "# List holds ID df for each year\n",
    "# Then concatenate dfs and drop duplicates to get unique IDs\n",
    "lgvs_ids = []\n",
    "car_diesel_ids = []\n",
    "car_petrol_ids = []\n",
    "car_hev_ids = []\n",
    "car_bev_ids = []\n",
    "mcycle_ids = []\n",
    "\n",
    "# Change dir to \"Sample_Data\" to get sample IDs - makes below code more readable\n",
    "print(\"Getting Sample IDs\")\n",
    "os.chdir(\"Sample_Data\")\n",
    "for year in range(2005, 2024):\n",
    "    print(year)\n",
    "    lgvs_ids.append(pd.read_parquet(f\"sample_IDs_lgv_{year}.parquet\", engine='pyarrow'))\n",
    "    car_diesel_ids.append(pd.read_parquet(f\"sample_IDs_car_diesel_{year}.parquet\", engine='pyarrow'))\n",
    "    car_petrol_ids.append(pd.read_parquet(f\"sample_IDs_car_petrol_{year}.parquet\", engine='pyarrow'))\n",
    "    car_hev_ids.append(pd.read_parquet(f\"sample_IDs_car_hev_{year}.parquet\", engine='pyarrow'))\n",
    "    car_bev_ids.append(pd.read_parquet(f\"sample_IDs_car_bev_{year}.parquet\", engine='pyarrow'))\n",
    "    mcycle_ids.append(pd.read_parquet(f\"sample_IDs_mcycle_{year}.parquet\", engine='pyarrow'))\n",
    "\n",
    "lgvs_ids = pd.concat(lgvs_ids).drop_duplicates()['vehicle_id'].to_list()\n",
    "car_diesel_ids = pd.concat(car_diesel_ids).drop_duplicates()['vehicle_id'].to_list()\n",
    "car_petrol_ids = pd.concat(car_petrol_ids).drop_duplicates()['vehicle_id'].to_list()\n",
    "car_hev_ids = pd.concat(car_hev_ids).drop_duplicates()['vehicle_id'].to_list()\n",
    "car_bev_ids = pd.concat(car_bev_ids).drop_duplicates()['vehicle_id'].to_list()\n",
    "mcycle_ids = pd.concat(mcycle_ids).drop_duplicates()['vehicle_id'].to_list()\n",
    "\n",
    "# Change dir back to script path\n",
    "os.chdir(data_path)\n",
    "\n",
    "# Print sample counts\n",
    "print(\"ID sample count\")\n",
    "print(\"LGV:\", len(lgvs_ids))\n",
    "print(\"Diesel Car:\", len(car_diesel_ids))\n",
    "print(\"Petrol Car:\", len(car_petrol_ids))\n",
    "print(\"HEV Car\", len(car_hev_ids))\n",
    "print(\"BEV Car\", len(car_bev_ids))\n",
    "print(\"Mcycle\", len(mcycle_ids))\n",
    "\n",
    "# Now get MOT data using sample IDs\n",
    "for year in tqdm(MOT_data_years):\n",
    "    print(year)\n",
    "\n",
    "    directory = str(year) + \"_Result\"\n",
    "    filename = f\"{year}_all_results.parquet\"\n",
    "    file_path = os.path.join(data_path, directory, filename)\n",
    "    print(f\"\\tInput: {filename}\")\n",
    "    df = pd.read_parquet(file_path, engine='pyarrow')\n",
    "\n",
    "    all_ids = [lgvs_ids, car_diesel_ids, car_petrol_ids, car_hev_ids, car_bev_ids, mcycle_ids]\n",
    "    veh_names = [\"lgv\", \"car_diesel\", \"car_petrol\", \"car_hev\", \"car_bev\", \"mcycle\"]\n",
    "    for veh_ids, veh_name in zip(all_ids, veh_names):\n",
    "        \n",
    "        # filter MOT data for IDs\n",
    "        df_filter = df[df['vehicle_id'].isin(veh_ids)]\n",
    "        print(f\"\\t\\t{veh_name}: {len(df_filter)}\")\n",
    "        directory = \"Sample_Data\"\n",
    "        filename = f\"sample_{veh_name}_MOT_{year}.parquet\"\n",
    "        file_path = os.path.join(data_path, directory, filename)\n",
    "        print(f\"\\t\\tOutput: {filename}\")\n",
    "        df_filter.to_parquet(file_path, engine='pyarrow', compression='snappy')\n",
    "        \n",
    "del [df, df_filter, lgvs_ids, car_diesel_ids, car_petrol_ids, car_hev_ids, car_bev_ids, mcycle_ids]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Aggregate data for Mileage & Scrappage Analysis + Transfromer Model\n",
    "\n",
    "fuel_lookup = {\n",
    "    \"car_hev\" : \"HY\",\n",
    "    \"car_bev\" : \"EL\",\n",
    "    \"car_diesel\" : \"DI\",\n",
    "    \"car_petrol\" : \"PE\",\n",
    "}\n",
    "\n",
    "veh_names = [\"car_diesel\", \"car_petrol\", \"car_hev\", \"car_bev\"]\n",
    "for veh_name in tqdm(veh_names):\n",
    "    print(veh_name)\n",
    "    \n",
    "    # Read in Data\n",
    "    years = list(range(2005, 2024, 1))\n",
    "    df = []\n",
    "    for year in years:\n",
    "        print(f\"Reading in MOT Data from: {year}\")\n",
    "        directory = \"Sample_Data\"\n",
    "        filename = f\"sample_{veh_name}_MOT_{year}.parquet\"\n",
    "        file_path = os.path.join(data_path, directory, filename)\n",
    "        df_year = pd.read_parquet(file_path, engine='pyarrow')\n",
    "        # drop cols that are not needed from now on\n",
    "        drop_cols = [\n",
    "            'Fuel Type', 'Manufacturer', 'Model', 'Engine Capacity', 'Year',\n",
    "            'cylinder_capacity_rounded', 'test_class_id',\n",
    "        ]\n",
    "        df_year.drop(columns=drop_cols, inplace=True)\n",
    "        \n",
    "        # Make sure that only correct fuel type is in the data\n",
    "        df_year = df_year[df_year['fuel_type'] == fuel_lookup[veh_name]]\n",
    "        \n",
    "        df.append(df_year)\n",
    "        \n",
    "    print(f\"Concatenating {veh_name} Samples from MOT Years\")    \n",
    "    df = pd.concat(df, ignore_index=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    del df_year\n",
    "    gc.collect()\n",
    "    \n",
    "    print(df.shape)\n",
    "    print(\"Sample before processign steps:\", df['vehicle_id'].nunique())\n",
    "    \n",
    "    n = df['vehicle_id'].nunique()\n",
    "    # Remove any vehicles that do not have 'cylinder_capacity' (excl. BEVs)\n",
    "    if veh_name != \"car_bev\":\n",
    "        ids = df.loc[(df['cylinder_capacity'].isna()) | (df['cylinder_capacity'] == 0) | (df['cylinder_capacity'] < 0), 'vehicle_id'].unique()\n",
    "        df = df[~df['vehicle_id'].isin(ids)]\n",
    "        new_n = df['vehicle_id'].nunique()\n",
    "        print(f\"Removed {n - new_n} vehicles with missing cylinder_capacity\")\n",
    "        n = new_n\n",
    "\n",
    "    # Ensure relevant columns are numeric\n",
    "    df['age_year'] = pd.to_numeric(df['age_year'], errors='coerce')\n",
    "    df['test_mileage'] = pd.to_numeric(df['test_mileage'], errors='coerce')\n",
    "\n",
    "    # Drop vehicle_ids with missing test_mileage or age - will create errors in later steps\n",
    "    n = df['vehicle_id'].nunique()\n",
    "    ids = df.loc[df['test_mileage'].isna(), 'vehicle_id'].unique()\n",
    "    df = df[~df['vehicle_id'].isin(ids)]\n",
    "    new_n = df['vehicle_id'].nunique()\n",
    "    print(f\"Removed {n - new_n} vehicles with missing test_mileage\")\n",
    "    n = new_n\n",
    "    ids = df.loc[df['age_year'].isna(), 'vehicle_id'].unique()\n",
    "    df = df[~df['vehicle_id'].isin(ids)]\n",
    "    new_n = df['vehicle_id'].nunique()\n",
    "    print(f\"Removed {n - new_n} vehicles with missing age_year\")\n",
    "    n = new_n\n",
    "\n",
    "    # Sort by vehicle ID and test date\n",
    "    print(\"Sorting by vehicle ID and test date\")\n",
    "    df = df.sort_values(by=['vehicle_id', 'test_date'])\n",
    "\n",
    "    print(\"Removing follow-up tests\")\n",
    "    # Next remove tests if they are not a pass, fail or PRS\n",
    "    # ABA and ABR result in another test - therefore just exlcude these\n",
    "    df = df[df['test_result'].isin(['P', 'F', 'PRS'])] \n",
    "    # Now get mileage per year, time between tests, etc\n",
    "    # Time between tests - in years\n",
    "    df[f\"previous_age_year\"] = df.groupby(\"vehicle_id\")[\"age_year\"].shift(1)\n",
    "    # For the first test for each vehicle_id set the previous_age_year to current age_year -1\n",
    "    df.loc[df[\"vehicle_id\"] != df[\"vehicle_id\"].shift(1), \"previous_age_year\"] = df[\"age_year\"] - 1\n",
    "    df[\"time_between_tests\"] = df[\"age_year\"] - df[\"previous_age_year\"]\n",
    "    # Now remove any tests that are too close together - likely due to a F followed by a P a few days later\n",
    "    df = df[(df['time_between_tests'] > 0.2) ] # remove tests that are less than 0.2 years (~ 2 months) apart\n",
    "    # Drop the new columns as these are recalculated later\n",
    "    df.drop(columns=['previous_age_year', 'time_between_tests'], inplace=True)\n",
    "    \n",
    "    # Remove any tests before 3 years old\n",
    "    df = df[df['age_year_rounded'] >= 3] # keep tests over 3 years old - assume first test is at 3 years old\n",
    "    \n",
    "    # Use shifts to calculate changes between tests\n",
    "    print(\"Calculating changes between tests\")\n",
    "    # Shift the data to find the changes between tests\n",
    "    df[f\"previous_test_mileage\"] = df.groupby(\"vehicle_id\")[\"test_mileage\"].shift(1)\n",
    "    df[f\"previous_test_date\"] = df.groupby(\"vehicle_id\")[\"test_date\"].shift(1)\n",
    "    df[f\"previous_age_year\"] = df.groupby(\"vehicle_id\")[\"age_year\"].shift(1)\n",
    "    df[f\"previous_age_year_rounded\"] = df.groupby(\"vehicle_id\")[\"age_year_rounded\"].shift(1) # probably not useful - need to round again later\n",
    "    # Check if the row is the first or last test in the data for each vehicle id using shift\n",
    "    df['last_test'] = df['vehicle_id'] != df['vehicle_id'].shift(-1)\n",
    "    df['first_test'] = df['vehicle_id'] != df['vehicle_id'].shift(1)\n",
    "    \n",
    "    # Now drop the first and last vehicle_id as the shift function will not work for these\n",
    "    df = df[df[\"vehicle_id\"] != df[\"vehicle_id\"].iloc[0]]\n",
    "    df = df[df[\"vehicle_id\"] != df[\"vehicle_id\"].iloc[-1]]\n",
    "    \n",
    "    # Now get mileage per year, time between tests, etc\n",
    "    # Time between tests - in years\n",
    "    df[\"time_between_tests\"] = df[\"age_year\"] - df[\"previous_age_year\"]\n",
    "    # Mileage between test\n",
    "    df[\"mileage_between_tests\"] = df[\"test_mileage\"] - df[\"previous_test_mileage\"]\n",
    "    # Change mileage_between_tests to float32 - needed for later calculation\n",
    "    df['mileage_between_tests'] = df['mileage_between_tests'].astype('float32') # \n",
    "    # Mileage per year\n",
    "    df[\"mileage_per_year\"] = df[\"mileage_between_tests\"] / df[\"time_between_tests\"]\n",
    "    \n",
    "    # Now perfrom some checks on these new columns\n",
    "    # Remove vehicles with mileage_between_tests < 0, i.e. input error for a test\n",
    "    n = df[\"vehicle_id\"].nunique()\n",
    "    ids = df.loc[(df[\"mileage_between_tests\"] < 0), \"vehicle_id\"].unique()\n",
    "    df = df[~df[\"vehicle_id\"].isin(ids)]\n",
    "    new_n = df[\"vehicle_id\"].nunique()\n",
    "    print(f\"Removed {n - new_n} vehicles with mileage_between_tests < 0\")\n",
    "    n = new_n\n",
    "    # Remove vehicles with mileage_per_year < 0, i.e. input error for a test - from time_between_tests being 0\n",
    "    ids = df.loc[(df[\"mileage_per_year\"] < 0), \"vehicle_id\"].unique()\n",
    "    df = df[~df[\"vehicle_id\"].isin(ids)]\n",
    "    new_n = df[\"vehicle_id\"].nunique()\n",
    "    print(f\"Removed {n - new_n} vehicles with mileage_per_year < 0\")\n",
    "    n = new_n\n",
    "    # Repeat  for time_between_tests - exclude vehicles with time_between_tests < 0\n",
    "    ids = df.loc[(df[\"time_between_tests\"] < 0), \"vehicle_id\"].unique()\n",
    "    df = df[~df[\"vehicle_id\"].isin(ids)]\n",
    "    new_n = df[\"vehicle_id\"].nunique()\n",
    "    print(f\"Removed {n - new_n} vehicles with time_between_tests < 0\")\n",
    "    \n",
    "    # Now correct the rows for the vehicle's first_test\n",
    "    df.loc[df[\"first_test\"] == True, \"mileage_per_year\"] = df[\"test_mileage\"] / df[\"age_year\"]\n",
    "    df.loc[df[\"first_test\"] == True, \"time_between_tests\"] = df[\"age_year\"]\n",
    "    df.loc[df[\"first_test\"] == True, \"mileage_between_tests\"] = df[\"test_mileage\"]  \n",
    "    # And add some corrections for these tests to make the data as if it has been 1 year since a previous test\n",
    "    # Edit the time between tests to 1.0 for first_test == True and age_year_rounded == 3 \n",
    "    df.loc[(df[\"age_year_rounded\"] == 3) & (df[\"first_test\"]==True), \"time_between_tests\"] = 1.0 \n",
    "    df.loc[(df[\"age_year_rounded\"] == 3) & (df[\"first_test\"]==True), \"mileage_between_tests\"] *= 0.333 # divide by 3\n",
    "    df.loc[(df[\"test_year\"] <= 2006) & (df[\"first_test\"]==True), \"time_between_tests\"] = 1.0 # set to 1.0 for before 2006 \n",
    "    \n",
    "    # Check for errors from the shifts\n",
    "    print(\"Checking for errors in the df\")\n",
    "    n = df[\"vehicle_id\"].nunique()\n",
    "    # Check for infinite values\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if np.isinf(df[numeric_cols]).values.any():\n",
    "        print(\"df contains infinite values. Replacing them with NaN.\")\n",
    "        df[numeric_cols] = df[numeric_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    new_n = df[\"vehicle_id\"].nunique()\n",
    "    print(f\"Removed {n - new_n} vehicles with infinite values\")\n",
    "    n = new_n\n",
    "           \n",
    "    # Remove any ids with NaN values in these columns\n",
    "    n = df[\"vehicle_id\"].nunique()\n",
    "    ids = df.loc[df['mileage_per_year'].isna(), 'vehicle_id'].unique()\n",
    "    df = df[~df['vehicle_id'].isin(ids)]\n",
    "    new_n = df[\"vehicle_id\"].nunique()\n",
    "    print(f\"Removed {n - new_n} vehicles with mileage per year < 100\")\n",
    "    n = new_n\n",
    "    ids = df.loc[df['time_between_tests'].isna(), 'vehicle_id'].unique()\n",
    "    df = df[~df['vehicle_id'].isin(ids)]\n",
    "    new_n = df[\"vehicle_id\"].nunique()\n",
    "    print(f\"Removed {n - new_n} vehicles with time_between_tests np.nan\")\n",
    "    n = new_n\n",
    "    ids = df.loc[df['mileage_between_tests'].isna(), 'vehicle_id'].unique()\n",
    "    df = df[~df['vehicle_id'].isin(ids)]\n",
    "    new_n = df[\"vehicle_id\"].nunique()\n",
    "    print(f\"Removed {n - new_n} vehicles with mileage_between_tests np.nan\")\n",
    "    n = new_n\n",
    "    \n",
    "    # Next round mileage cols to ints\n",
    "    df['test_mileage'] = df['test_mileage'].round(0).astype('int32')\n",
    "    df['mileage_between_tests'] = df['mileage_between_tests'].round(0).astype('int32')\n",
    "    df['mileage_per_year'] = df['mileage_per_year'].round(0).astype('int32')\n",
    "    \n",
    "    # Next remove any vehicle that has less than 100 miles per year - likely input error missing a digit\n",
    "    n = df[\"vehicle_id\"].nunique()\n",
    "    ids = df.loc[df['mileage_per_year'] < 100, 'vehicle_id'].unique()\n",
    "    \n",
    "    df = df[~df['vehicle_id'].isin(ids)]\n",
    "    new_n = df[\"vehicle_id\"].nunique()\n",
    "    print(f\"Removed {n - new_n} vehicles with mileage per year < 100\")\n",
    "    n = new_n\n",
    "    \n",
    "    # Now add estimated data for the first and second years based on the first test\n",
    "    # Year 1 is 1/3 of the third test and year 2 is 2/3 of the third test\n",
    "    # New rows for years 1 and 2\n",
    "    print(\"Adding Rows for Years 0,  1 and 2\")\n",
    "    \n",
    "    # keep only vehicles that still have their first test\n",
    "    ids = df.loc[df[\"first_test\"]==True, \"vehicle_id\"].unique()\n",
    "    df = df[df[\"vehicle_id\"].isin(ids)]\n",
    "    new_n = df[\"vehicle_id\"].nunique()\n",
    "    print(f\"Removed {n - new_n} vehicles with first test\")\n",
    "    n = new_n\n",
    "    \n",
    "    df_year_3 = df[(df[\"age_year_rounded\"] == 3) & (df[\"first_test\"] == True)]\n",
    "    \n",
    "    # Create a new dfframe with the new rows for years 1 and 2\n",
    "    # Data used for transformer model and CO2 calculations only\n",
    "    # Year 2\n",
    "    df_year_2 = df_year_3.copy()\n",
    "    df_year_2[\"age_year_rounded\"] = 2\n",
    "    df_year_2[\"mileage_per_year\"] = df_year_2[\"mileage_per_year\"]\n",
    "    df_year_2[\"test_mileage\"] = 2 * df_year_2[\"mileage_per_year\"]\n",
    "    df_year_2[\"test_date\"] = df_year_2[\"test_date\"] - dt.timedelta(days=365)\n",
    "    df_year_2[\"test_year\"] = df_year_2[\"test_year\"] - 1\n",
    "    df_year_2[\"time_between_tests\"] = 1\n",
    "    df_year_2[\"age_year\"] = df_year_2[\"age_year\"] - 1\n",
    "    df_year_2[\"age_year_rounded\"] = 2\n",
    "    df_year_2[\"mileage_between_tests\"] = df_year_2[\"mileage_per_year\"]\n",
    "    df_year_2[\"last_test\"] = False\n",
    "    df_year_2[\"first_test\"] = False\n",
    "    # Year 1\n",
    "    df_year_1 = df_year_3.copy()\n",
    "    df_year_1[\"age_year_rounded\"] = 1\n",
    "    df_year_1[\"mileage_per_year\"] = df_year_1[\"mileage_per_year\"]\n",
    "    df_year_1[\"test_mileage\"] = df_year_1[\"mileage_per_year\"]\n",
    "    df_year_1[\"test_date\"] = df_year_1[\"test_date\"] - dt.timedelta(days=2*365)\n",
    "    df_year_1[\"test_year\"] = df_year_1[\"test_year\"] - 2\n",
    "    df_year_1[\"time_between_tests\"] = 1\n",
    "    df_year_1[\"age_year\"] = df_year_1[\"age_year\"] - 2\n",
    "    df_year_1[\"age_year_rounded\"] = 1\n",
    "    df_year_1[\"mileage_between_tests\"] = df_year_1[\"mileage_per_year\"]\n",
    "    df_year_1[\"last_test\"] = False\n",
    "    df_year_1[\"first_test\"] = False\n",
    "    # Year 0 - ie the point of manufacture - used for LCA emissions for manufacture\n",
    "    df_year_0 = df_year_3.copy()\n",
    "    df_year_0[\"age_year_rounded\"] = 0\n",
    "    df_year_0[\"mileage_per_year\"] = 0\n",
    "    df_year_0[\"test_mileage\"] = 0\n",
    "    df_year_0[\"test_date\"] = df_year_0[\"first_use_date\"]\n",
    "    df_year_0[\"test_year\"] = df_year_0[\"first_use_year\"]\n",
    "    df_year_0[\"time_between_tests\"] = 0\n",
    "    df_year_0[\"age_year\"] = 0\n",
    "    df_year_0[\"age_year_rounded\"] = 0\n",
    "    df_year_0[\"mileage_between_tests\"] = 0\n",
    "    df_year_0[\"last_test\"] = False\n",
    "    df_year_0[\"first_test\"] = False\n",
    "    # Now add these new rows to the df\n",
    "    df = pd.concat([df, df_year_2, df_year_1, df_year_0], ignore_index=True)\n",
    "    df = df.sort_values(by=['vehicle_id', 'test_date'])\n",
    "    \n",
    "    # drop cols that are not used further\n",
    "    drop_cols = ['previous_test_mileage', 'previous_test_date', 'previous_age_year', 'previous_age_year_rounded', 'test_result']\n",
    "    df.drop(columns=drop_cols, inplace=True)\n",
    "    \n",
    "    # Finished processing data     \n",
    "    print(f\"Memory usage for dataset- {df.memory_usage().sum() / 1e6:.2f} MB\")\n",
    "    print(df.info())\n",
    "    print(df.describe().to_string())\n",
    "    directory = \"Sample_Data\" \n",
    "    filename = f\"sample_{veh_name}_all_years.parquet\"\n",
    "    file_path = os.path.join(data_path, directory, filename)\n",
    "    df.to_parquet(file_path, engine='pyarrow', compression='snappy')\n",
    "    \n",
    "    print(\"Sample after processing steps:\", df['vehicle_id'].nunique())\n",
    "    \n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Prepare Data for Transformer model:\n",
    "\n",
    "fuel_lookup = {\n",
    "    \"car_hev\" : \"HY\",\n",
    "    \"car_bev\" : \"EL\",\n",
    "    \"car_diesel\" : \"DI\",\n",
    "    \"car_petrol\" : \"PE\",\n",
    "}\n",
    "\n",
    "print(\"Preparing training data...\")\n",
    "for veh_name in [\"car_diesel\", \"car_petrol\"]:\n",
    "    \n",
    "    # Only keep the columns we need for training\n",
    "    categorical_cols = ['fuel_type', 'last_test']\n",
    "    numerical_cols = ['mileage_per_year', 'test_mileage', 'age_year', 'time_between_tests']\n",
    "    training_cols = ['vehicle_id', 'test_year', 'first_test', 'first_use_year', 'age_year_rounded'] + categorical_cols + numerical_cols\n",
    "    \n",
    "    print(f\"\\tReading in {veh_name} data...\")\n",
    "    directory = \"Sample_Data\" \n",
    "    filename = f\"sample_{veh_name}_all_years.parquet\"\n",
    "    file_path = os.path.join(data_path, directory, filename)\n",
    "    data = pd.read_parquet(file_path, engine='pyarrow', columns=training_cols)\n",
    "    \n",
    "    # Make sure that only correct fuel type is in the data\n",
    "    ids = data.loc[data['fuel_type'] != fuel_lookup[veh_name], 'vehicle_id'].unique()\n",
    "    data = data[~data['vehicle_id'].isin(ids)]\n",
    "    \n",
    "    \n",
    "    # Add a few filtering steps to keep data suitable for training\n",
    "    print(f\"\\tFiltering {veh_name} Data for Training. Initial Sample: {data['vehicle_id'].nunique()}\")\n",
    "    n = data[\"vehicle_id\"].nunique()\n",
    "    \n",
    "    ids = data.loc[data.isnull().any(axis=1), 'vehicle_id'].unique()\n",
    "    data = data[~data['vehicle_id'].isin(ids)]\n",
    "    new_n = data[\"vehicle_id\"].nunique()\n",
    "    print(f\"\\tRemoved {n - new_n} vehicles with NaN values\")\n",
    "    n = new_n\n",
    "    \n",
    "    # Next drop any vehicle that has over 2 Years between tests - outliers\n",
    "    ids = data.loc[data['time_between_tests'] > 2, 'vehicle_id'].unique()\n",
    "    data = data[~data['vehicle_id'].isin(ids)]\n",
    "    new_n = data[\"vehicle_id\"].nunique()\n",
    "    print(f\"\\tRemoved {n - new_n} vehicles with time between tests > 2 years\")\n",
    "    n = new_n\n",
    "    \n",
    "    # Next remove nay vehicle that has over 100,000 miles per year - outliers\n",
    "    ids = data.loc[data['mileage_per_year'] > 100_000, 'vehicle_id'].unique()\n",
    "    data = data[~data['vehicle_id'].isin(ids)]\n",
    "    new_n = data[\"vehicle_id\"].nunique()\n",
    "    print(f\"\\tRemoved {n - new_n} vehicles with mileage per year > 100,000\")\n",
    "    n = new_n\n",
    "    \n",
    "    # Sample selection for Transformer model:\n",
    "    # 1. Get the survival rate distribtion from Figture 3 \n",
    "    # 2. Sample is then derived from the proportions of vehciles exitting the fleet at each age\n",
    "    # 3. Use the survial rate to find the maxium Transformer sample size possible, \n",
    "    #    given the already selected MOT data sample size\n",
    "    \n",
    "    # Survival rate dist:\n",
    "    fpath = os.path.join(data_path, \"survival_rate_data\", \"figure_3_plot_c_top.csv\")\n",
    "    df_dist = pd.read_csv(fpath, index_col=0)\n",
    "    df_dist = df_dist[df_dist['fuel'] == veh_name]\n",
    "    ages = df_dist['age'].to_list()\n",
    "    surv_rate = df_dist['percentage'].to_list()\n",
    "    \n",
    "    # get the proportion between subsequent ages\n",
    "    surv_rate_diff = []\n",
    "    for i in range(1, len(surv_rate)):\n",
    "        surv_rate_diff.append(surv_rate[i-1] - surv_rate[i])\n",
    "    ages = ages[1:] # not enough data for age == 3\n",
    "    surv_rate_diff = [s for s in surv_rate_diff if s > 0.0] # remove negative values\n",
    "\n",
    "    # Find the maxium sample size possible for this survival rate curve and MOT data sample size\n",
    "    # only using vehciles that left the fleet beteween 2015 and 2021\n",
    "    first_age = True\n",
    "    for age, surv in zip(ages, surv_rate_diff):\n",
    "        n_ids = data.loc[(data['age_year_rounded'] == age) & (data['last_test'] == True) & (data['test_year'].isin([2015, 2016, 2017, 2018, 2019, 2020, 2021])), 'vehicle_id'].nunique()\n",
    "        \n",
    "        max_sample_size_for_age = n_ids / surv\n",
    "        print(f\"\\tVehicles for Age {age}, Surival Rate {round(surv, 3)}, ID's {n_ids}, Implied Sample Size {int(max_sample_size_for_age)}\")\n",
    "        \n",
    "        if first_age:\n",
    "            sample_size = max_sample_size_for_age\n",
    "            sample_age_limiter = age\n",
    "            first_age = False\n",
    "        else:\n",
    "            if max_sample_size_for_age < sample_size:\n",
    "                sample_age_limiter = age\n",
    "            sample_size = min(sample_size, max_sample_size_for_age)\n",
    "            \n",
    "    print(f\"\\tTotal Sample Size: {sample_size}, Limited by Age: {sample_age_limiter}\")\n",
    "    print(f\"\\tTotal Sample Size: {sample_size} * 2 = {sample_size * 2}\") # multiply this by 2 since the limiting age is always much lower (~1/2) than the rest\n",
    "    \n",
    "    # Double the sample size since the limiting age is always much lower (~1/2) than the rest\n",
    "    sample_size *= 2 \n",
    "    \n",
    "    print(\"Selecting Sample for Transformer Model\")\n",
    "    # Now select the vehicles for the sample using sample_size\n",
    "    ids = []\n",
    "    for age, surv in zip(ages, surv_rate_diff):\n",
    "        print(f\"\\tAge: {age}, Survival Rate: {surv}, Sample Size: {int(sample_size * surv)}\")\n",
    "        ids_age = data.loc[(data['age_year_rounded'] == age) & (data['last_test'] == True) & (data['test_year'].isin([2015, 2016, 2017, 2018, 2019, 2020, 2021])), 'vehicle_id'].unique()\n",
    "        n_age = int(sample_size * surv)\n",
    "        try:\n",
    "            ids += list(np.random.choice(ids_age, size=n_age, replace=False))\n",
    "        except:\n",
    "            print(f\"\\tWarning - number of ids less than sample size {age}, {surv} : {n_age}\")\n",
    "            print(f\"\\tAdding all ids for this age: {len(ids_age)}\")\n",
    "            ids += list(ids_age)\n",
    "    data = data[data['vehicle_id'].isin(ids)]\n",
    "    new_n = data[\"vehicle_id\"].nunique()\n",
    "    print(f\"\\tRemoved {n - new_n} vehicles to fit survival rate distribution\")\n",
    "    n = new_n      \n",
    "\n",
    "    print(f\"\\tFiltered {veh_name} Sample: {data['vehicle_id'].nunique()}\")\n",
    "    print(f\"Memory usage for dataset- {data.memory_usage().sum() / 1e6:.2f} MB\")\n",
    "    print(data.info())\n",
    "    print(data.describe().to_string())\n",
    "    \n",
    "    folder = \"Sample_Data\"\n",
    "    filename = f\"transformer_training_data_{veh_name}.parquet\"\n",
    "    fpath = os.path.join(data_path, folder, filename)\n",
    "    data = data.to_parquet(fpath, compression='snappy', engine='pyarrow')\n",
    "    \n",
    "    del data\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Prepare Data for Predictions with the transformer model\n",
    "data_fuels = []\n",
    "for veh_name in [\"car_diesel\", \"car_petrol\", \"car_bev\", \"car_hev\"]:\n",
    "    print(f\"Reading in {veh_name} data...\")\n",
    "    directory = \"Sample_Data\" \n",
    "    filename = f\"sample_{veh_name}_all_years.parquet\"\n",
    "    file_path = os.path.join(data_path, directory, filename)\n",
    "    data = pd.read_parquet(file_path, engine='pyarrow')\n",
    "    \n",
    "    # Only keep the columns we need for predictions\n",
    "    categorical_cols = ['fuel_type', 'last_test']\n",
    "    numerical_cols = ['mileage_per_year', 'test_mileage', 'age_year', 'time_between_tests']#, 'test_mileage_age_indicator', 'mileage_per_year_age_indicator', 'taxi_indicator']\n",
    "    data_cols = ['vehicle_id', 'test_year', 'first_test', 'make', 'model',\n",
    "                     'first_use_year', 'fuel efficiency Wh/mi', 'battery capacity (kWh)', \n",
    "                     'CO2 g/km', 'mass (kg)']\n",
    "    predictions_cols = data_cols + categorical_cols + numerical_cols\n",
    "    data = data[predictions_cols]\n",
    "\n",
    "    # Add a few filtering steps\n",
    "    n = data['vehicle_id'].nunique()\n",
    "    print(f\"\\tFiltering {veh_name} Data for Training. Initial Sample: {n}\")\n",
    "    \n",
    "    # remove any ids with NaN for any of the categorical or numerical cols\n",
    "    ids = data[categorical_cols + numerical_cols + ['vehicle_id']].loc[data[categorical_cols + numerical_cols + ['vehicle_id']].isnull().any(axis=1), 'vehicle_id'].unique()\n",
    "    data = data[~data['vehicle_id'].isin(ids)]\n",
    "    new_n = data['vehicle_id'].nunique()\n",
    "    print(f\"\\tRemoved {n - new_n} vehicles with NaN values\")\n",
    "    n = new_n\n",
    "    \n",
    "    # Next drop any vehicle that has over 2 Years between tests\n",
    "    ids = data.loc[data['time_between_tests'] > 2, 'vehicle_id'].unique()\n",
    "    data = data[~data['vehicle_id'].isin(ids)]\n",
    "    new_n = data['vehicle_id'].nunique()\n",
    "    print(f\"\\tRemoved {n - new_n} vehicles with time between tests > 2\")\n",
    "    n = new_n\n",
    "    \n",
    "    # Check that all vehicles now have at least 3 records of data - min sequence length is 3\n",
    "    vehicle_counts = data['vehicle_id'].value_counts()\n",
    "    ids = vehicle_counts[vehicle_counts < 3].index\n",
    "    data = data[~data['vehicle_id'].isin(ids)]\n",
    "    new_n = data['vehicle_id'].nunique()\n",
    "    print(f\"\\tRemoved {n - new_n} vehicles with less than 3 records\")\n",
    "    n = new_n\n",
    "    \n",
    "    # Next remove nay vehicle that has over 100,000 miles per year\n",
    "    ids = data.loc[data['mileage_per_year'] > 100_000, 'vehicle_id'].unique()\n",
    "    data = data[~data['vehicle_id'].isin(ids)]\n",
    "    new_n = data['vehicle_id'].nunique()\n",
    "    print(f\"\\tRemoved {n - new_n} vehicles with mileage per year > 100,000\")\n",
    "    n = new_n\n",
    "        \n",
    "    # Sample selection\n",
    "    ids = []\n",
    "    # Find the number of vehicle_ids in the fleet for each first_use year\n",
    "    for year in range(2005, 2021, 1):\n",
    "        n = data.loc[data['first_use_year'] == year, 'vehicle_id'].nunique()\n",
    "        print(f\"\\tFirst Use Year: {year}, Vehicles: {n}\")   \n",
    "        \n",
    "        # Now select 50_000 from each year, except for car_bev use all ids\n",
    "        ids_year = data.loc[data['first_use_year'] == year, 'vehicle_id'].unique()\n",
    "        if veh_name == 'car_bev':\n",
    "            ids += list(ids_year)\n",
    "        else:\n",
    "            try:\n",
    "                ids += list(np.random.choice(ids_year, size=50_000, replace=False))\n",
    "            except:\n",
    "                print(f\"\\tWarning: with sample size {year} : {50_000}\")\n",
    "                print(f\"\\tAdding all ids for this year: {len(ids_year)}\")\n",
    "                ids += list(ids_year)\n",
    "    data = data[data['vehicle_id'].isin(ids)]\n",
    "    new_n = data['vehicle_id'].nunique()\n",
    "    print(f\"\\tFiltered {veh_name} Sample: {new_n}\")\n",
    "    print(f\"\\tFiltered {veh_name} Sample: {data['vehicle_id'].nunique()}\")\n",
    "    folder = \"Sample_Data\"\n",
    "    filename = f\"transformer_prediction_data_{veh_name}.parquet\"\n",
    "    file_path = os.path.join(data_path, folder, filename)\n",
    "    data = data.to_parquet(file_path, compression='snappy', engine='pyarrow')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
