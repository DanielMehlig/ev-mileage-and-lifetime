{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Set up \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Note see MOT_transformer_model_module for the full list of imports\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mehlig\\miniconda3\\envs\\ML\\Lib\\site-packages\\pandas\\__init__.py:14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _dependency \u001b[38;5;129;01min\u001b[39;00m _hard_dependencies:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 14\u001b[0m         \u001b[38;5;28m__import__\u001b[39m(_dependency)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _e:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m     16\u001b[0m         _missing_dependencies\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_dependency\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_e\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mehlig\\miniconda3\\envs\\ML\\Lib\\site-packages\\numpy\\__init__.py:149\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m exceptions\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[1;32m--> 149\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lib\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# NOTE: to be revisited following future namespace cleanup.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# See gh-14454 and gh-15672 for discussion.\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mehlig\\miniconda3\\envs\\ML\\Lib\\site-packages\\numpy\\lib\\__init__.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Public submodules\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Note: recfunctions and (maybe) format are public too, but not imported\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mixins\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m scimath \u001b[38;5;28;01mas\u001b[39;00m emath\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Private submodules\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# load module names. See https://github.com/networkx/networkx/issues/5838\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m type_check\n",
      "File \u001b[1;32mc:\\Users\\mehlig\\miniconda3\\envs\\ML\\Lib\\site-packages\\numpy\\lib\\scimath.py:441\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_power_dispatcher\u001b[39m(x, p):\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (x, p)\n\u001b[1;32m--> 441\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_power_dispatcher)\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpower\u001b[39m(x, p):\n\u001b[0;32m    443\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;124;03m    Return x to the power p, (x**p).\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    479\u001b[0m \n\u001b[0;32m    480\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    481\u001b[0m     x \u001b[38;5;241m=\u001b[39m _fix_real_lt_zero(x)\n",
      "File \u001b[1;32mc:\\Users\\mehlig\\miniconda3\\envs\\ML\\Lib\\site-packages\\numpy\\core\\overrides.py:161\u001b[0m, in \u001b[0;36marray_function_dispatch.<locals>.decorator\u001b[1;34m(implementation)\u001b[0m\n\u001b[0;32m    158\u001b[0m     add_docstring(implementation, dispatcher\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m)\n\u001b[0;32m    160\u001b[0m public_api \u001b[38;5;241m=\u001b[39m _ArrayFunctionDispatcher(dispatcher, implementation)\n\u001b[1;32m--> 161\u001b[0m public_api \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mwraps(implementation)(public_api)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    164\u001b[0m     public_api\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m=\u001b[39m module\n",
      "File \u001b[1;32mc:\\Users\\mehlig\\miniconda3\\envs\\ML\\Lib\\functools.py:58\u001b[0m, in \u001b[0;36mupdate_wrapper\u001b[1;34m(wrapper, wrapped, assigned, updated)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;28msetattr\u001b[39m(wrapper, attr, value)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m updated:\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(wrapper, attr)\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mgetattr\u001b[39m(wrapped, attr, {}))\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Issue #17482: set __wrapped__ last so we don't inadvertently copy it\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# from the wrapped function when updating __dict__\u001b[39;00m\n\u001b[0;32m     61\u001b[0m wrapper\u001b[38;5;241m.\u001b[39m__wrapped__ \u001b[38;5;241m=\u001b[39m wrapped\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set up \n",
    "# Note see MOT_transformer_model_module for the full list of imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Enable auto updates from imported modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(os.getcwd)\n",
    "os.chdir(\"..\")\n",
    "script_path = os.getcwd()\n",
    "data_path = os.path.join(script_path, \"data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read in training data\n",
    "\n",
    "print(\"Reading in training data...\")\n",
    "data_fuels = []\n",
    "for veh_name in [\"car_diesel\", \"car_petrol\"]:\n",
    "    print(f\"\\tReading in {veh_name} data...\")\n",
    "    fpath = os.path.join(data_path, \"Sample_Data\", f\"transformer_training_data_{veh_name}.parquet\")\n",
    "    data = pd.read_parquet(fpath, engine='pyarrow')\n",
    "    \n",
    "    # Only keep the columns we need for training\n",
    "    categorical_cols = ['fuel_type', 'last_test', ]\n",
    "    numerical_cols = ['mileage_per_year', 'test_mileage', 'age_year', 'time_between_tests']\n",
    "    training_cols = ['vehicle_id'] + categorical_cols + numerical_cols\n",
    "    data = data[training_cols]\n",
    "    \n",
    "    print(f\"\\tSample: {data['vehicle_id'].nunique()}\")\n",
    "\n",
    "    data_fuels.append(data)\n",
    "    \n",
    "data = pd.concat(data_fuels, ignore_index=True)\n",
    "\n",
    "print(\"Combined Sample: \", data['vehicle_id'].nunique())\n",
    "\n",
    "del data_fuels\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Data loaded. Shape: {data.shape}\")\n",
    "print(data.describe().to_string())\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Prepare training data \n",
    "\n",
    "from MOT_transformer_model_module import prepare_training_data, validate_dataset\n",
    "\n",
    "batch_size= 15_000 \n",
    "\n",
    "# Prepare data \n",
    "train_dataset, test_dataset, label_encoders, scaler = prepare_training_data(\n",
    "    data, \n",
    "    test_size=0.2, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True\n",
    "    )\n",
    "test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "print(\"Validating training data:\")\n",
    "validate_dataset(train_loader)\n",
    "print(\"\\nValidating test data:\")\n",
    "validate_dataset(test_loader)\n",
    "print()\n",
    "\n",
    "# Save label encoders and scaler\n",
    "torch.save(label_encoders, \"label_encoders.pt\")\n",
    "torch.save(scaler, \"scaler.pt\")\n",
    "\n",
    "# Save the training and test data loaders\n",
    "torch.save(train_loader, \"train_loader.pt\")\n",
    "torch.save(test_loader, \"test_loader.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Set up model \n",
    "\n",
    "from MOT_transformer_model_module import VehicleTransformer\n",
    "\n",
    "# Set up model\n",
    "\n",
    "# Model parameters\n",
    "input_dim = len(data.columns) - 1  # Subtract 1 for vehicle_id column that is dropped during sequence creation\n",
    "d_model = 128\n",
    "nhead = 8\n",
    "num_layers = 6\n",
    "dim_feedforward = 256\n",
    "num_epochs = 2\n",
    "\n",
    "# Initialize model\n",
    "model = VehicleTransformer(\n",
    "    input_dim=input_dim,\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    num_layers=num_layers,\n",
    "    dim_feedforward=dim_feedforward\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Train model\n",
    "\n",
    "from MOT_transformer_model_module import train_model\n",
    "\n",
    "# load the loaders\n",
    "train_loader = torch.load(\"train_loader.pt\")\n",
    "test_loader = torch.load(\"test_loader.pt\")\n",
    "\n",
    "train_metrics, val_metrics, best_threshold, threshold_results = train_model(\n",
    "    model, train_loader, test_loader, num_epochs=num_epochs, device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Analyze model predictions on test data\n",
    "\n",
    "from MOT_transformer_model_module import analyze_model_predictions, transformer_figure\n",
    "\n",
    "# Load the loader, label encoders and scaler - don't need to rerun previous cells\n",
    "test_loader = torch.load(\"test_loader.pt\")\n",
    "label_encoders = torch.load(\"label_encoders.pt\")\n",
    "scaler = torch.load(\"scaler.pt\")\n",
    "\n",
    "\n",
    "results = analyze_model_predictions(model, test_loader, scaler, device, best_threshold=0.32)\n",
    "fig = transformer_figure(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Read in data for predictions:\n",
    "\n",
    "print(\"Reading in prediction data...\")\n",
    "data_fuels = []\n",
    "for veh_name in [\"car_diesel\", \"car_petrol\", \"car_bev\", \"car_hev\"]:\n",
    "    print(f\"\\tReading in {veh_name} data...\")\n",
    "    fpath = os.path.join(data_path, \"Sample_Data\", f\"transformer_prediction_data_{veh_name}.parquet\")\n",
    "    data = pd.read_parquet(fpath, engine='pyarrow')\n",
    "    \n",
    "    # Only keep the columns we need for predictions\n",
    "    categorical_cols = ['fuel_type', 'last_test']\n",
    "    numerical_cols = ['mileage_per_year', 'test_mileage', 'age_year', 'time_between_tests']#, 'test_mileage_age_indicator', 'mileage_per_year_age_indicator', 'taxi_indicator']\n",
    "    predictions_cols = ['vehicle_id', 'test_year', 'make', 'model',\n",
    "                     'first_use_year', 'fuel efficiency Wh/mi', 'battery capacity (kWh)',\n",
    "                     'CO2 g/km', 'mass (kg)'\n",
    "                     ] + categorical_cols + numerical_cols\n",
    "    data = data[predictions_cols]\n",
    "    \n",
    "    data_fuels.append(data)\n",
    "    \n",
    "data = pd.concat(data_fuels, ignore_index=True)\n",
    "\n",
    "print(\"Combined Sample: \", data['vehicle_id'].nunique())\n",
    "\n",
    "# Next show the distribution of fuel types\n",
    "for fuel in [\"DI\", \"PE\", \"EL\", \"HY\"]:\n",
    "    n = data.loc[data['fuel_type'] == fuel, 'vehicle_id'].nunique()\n",
    "    print(f\"{fuel}: {n} vehicles\")\n",
    "    \n",
    "# Change \"EL\" and \"HY\" to \"DI\" or \"PE\" based upon mileage\n",
    "# First create a copy of the original fuel type column\n",
    "data[\"original_fuel_type\"] = data[\"fuel_type\"]\n",
    "# Catgorise EL and HY based upon mileage criteria:\n",
    "for first_use_year in range(2005, 2021, 1):\n",
    "    di_mileage = data.loc[(data[\"fuel_type\"] == \"DI\") & (data[\"first_use_year\"] == first_use_year) & (data[\"last_test\"] == True), \"test_mileage\"].mean()\n",
    "    pe_mileage = data.loc[(data[\"fuel_type\"] == \"PE\") & (data[\"first_use_year\"] == first_use_year) & (data[\"last_test\"] == True), \"test_mileage\"].mean()\n",
    "    cut_off = (di_mileage + pe_mileage) / 2\n",
    "    print(f\"Cut-off for {first_use_year}: {round(cut_off)} miles\")\n",
    "\n",
    "    # Get the ids of the HY and EL that are now DI and PE for this first_use_year\n",
    "    el_di_ids = data.loc[(data[\"fuel_type\"] == \"EL\") & (data[\"first_use_year\"] == first_use_year) & (data[\"last_test\"] == True) & (data[\"test_mileage\"] >= cut_off), \"vehicle_id\"].unique()\n",
    "    el_pe_ids = data.loc[(data[\"fuel_type\"] == \"EL\") & (data[\"first_use_year\"] == first_use_year) & (data[\"last_test\"] == True) & (data[\"test_mileage\"] < cut_off), \"vehicle_id\"].unique()\n",
    "    hy_di_ids = data.loc[(data[\"fuel_type\"] == \"HY\") & (data[\"first_use_year\"] == first_use_year) & (data[\"last_test\"] == True) & (data[\"test_mileage\"] >= cut_off), \"vehicle_id\"].unique()\n",
    "    hy_pe_ids = data.loc[(data[\"fuel_type\"] == \"HY\") & (data[\"first_use_year\"] == first_use_year) & (data[\"last_test\"] == True) & (data[\"test_mileage\"] < cut_off), \"vehicle_id\"].unique()\n",
    "    \n",
    "    # Now set these ids as DI or PE\n",
    "    data.loc[data[\"vehicle_id\"].isin(el_di_ids), \"fuel_type\"] = \"DI\"\n",
    "    data.loc[data[\"vehicle_id\"].isin(el_pe_ids), \"fuel_type\"] = \"PE\"\n",
    "    data.loc[data[\"vehicle_id\"].isin(hy_di_ids), \"fuel_type\"] = \"DI\"\n",
    "    data.loc[data[\"vehicle_id\"].isin(hy_pe_ids), \"fuel_type\"] = \"PE\"\n",
    "    \n",
    "# Finally check if there are any vehicles that have not been reclassified\n",
    "n_el = data.loc[data[\"fuel_type\"] == \"EL\", \"vehicle_id\"].nunique()\n",
    "n_hy = data.loc[data[\"fuel_type\"] == \"HY\", \"vehicle_id\"].nunique()\n",
    "print(f\"EL: {n_el} vehicles not reclassified\")\n",
    "print(f\"HY: {n_hy} vehicles not reclassified\")\n",
    "# defualt these to DI\n",
    "data.loc[data[\"fuel_type\"] == \"EL\", \"fuel_type\"] = \"DI\"\n",
    "data.loc[data[\"fuel_type\"] == \"HY\", \"fuel_type\"] = \"DI\"\n",
    "\n",
    "# Now print the number of ELs now classified as DI and PE\n",
    "el_de = data.loc[(data[\"original_fuel_type\"] == \"EL\") &\n",
    "                (data[\"fuel_type\"] == \"DI\"), \"vehicle_id\"].nunique()\n",
    "el_pe = data.loc[(data[\"original_fuel_type\"] == \"EL\") &\n",
    "                (data[\"fuel_type\"] == \"PE\"), \"vehicle_id\"].nunique()\n",
    "hy_de = data.loc[(data[\"original_fuel_type\"] == \"HY\") &\n",
    "                (data[\"fuel_type\"] == \"DI\"), \"vehicle_id\"].nunique()\n",
    "hy_pe = data.loc[(data[\"original_fuel_type\"] == \"HY\") &\n",
    "                (data[\"fuel_type\"] == \"PE\"), \"vehicle_id\"].nunique()\n",
    "de_original = data.loc[data[\"original_fuel_type\"] == \"DI\", \"vehicle_id\"].nunique()\n",
    "pe_original = data.loc[data[\"original_fuel_type\"] == \"PE\", \"vehicle_id\"].nunique()\n",
    "print(f\"EL: {el_de} DI, {el_pe} PE\")\n",
    "print(f\"HY: {hy_de} DI, {hy_pe} PE\")\n",
    "print(f\"DI: {de_original} original, {el_de + hy_de} reclassified\")\n",
    "print(f\"PE: {pe_original} original, {el_pe + hy_pe} reclassified\")\n",
    "\n",
    "# Finally reset last_test for all tests after 2022 to be False\n",
    "data.loc[data['test_year'] >= 2022, 'last_test'] = False\n",
    "\n",
    "del data_fuels\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Data loaded. Shape: {data.shape}\")\n",
    "print(data.describe().to_string())\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Predctions\n",
    "from MOT_transformer_model_module import generate_predictions\n",
    "\n",
    "# Load the saved state dict\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()  # Put the model in evaluation mode\n",
    "\n",
    "# Load scaler and label encoders - don't need to run earlier code if loading from here\n",
    "scaler = torch.load(\"scaler.pt\")\n",
    "label_encoders = torch.load(\"label_encoders.pt\")\n",
    "\n",
    "data['simulated_data'] = False\n",
    "data['scrap_probability'] = 0.0\n",
    "\n",
    "# First remove vehicles that have already been scrapped\n",
    "ids = data.loc[data[\"last_test\"]==True, \"vehicle_id\"].unique()\n",
    "scrapped_data = data[data[\"vehicle_id\"].isin(ids)] \n",
    "data = data[~data[\"vehicle_id\"].isin(ids)]\n",
    "# output the scrapped data\n",
    "fname = \"simulation_data_0.csv\"\n",
    "fpath = os.path.join(\"simulation_data\", fname)\n",
    "scrapped_data.to_csv(fpath, index=False)\n",
    "\n",
    "# Simulate the next test for all vehicles  \n",
    "i = 1\n",
    "while len(data) > 1:\n",
    "    print(f\"\\nIteration {i}\")\n",
    "    print(f\"Data shape: {data.shape}\")\n",
    "    print(f\"Unique vehicles: {data['vehicle_id'].nunique()}\")\n",
    "    \n",
    "    # Generate predictions for next tests\n",
    "    # batch size much larger than training as we are not backpropagating\n",
    "    new_data = generate_predictions(model, data, label_encoders, scaler, device, batch_size=50_000, threshold=0.22)\n",
    "    \n",
    "    # Concatenate with existing data and sort\n",
    "    data = pd.concat([data, new_data], ignore_index=True)\n",
    "    data = data.sort_values(['vehicle_id', 'test_year']).reset_index(drop=True)\n",
    "    \n",
    "    # Filter out vehicles that have been predicted to be scrapped and export them\n",
    "    scrapped_ids = data.loc[data[\"last_test\"]==True, \"vehicle_id\"].unique()\n",
    "    data_scrapped = data[data[\"vehicle_id\"].isin(scrapped_ids)]\n",
    "    fname = f\"simulation_data_{i}.csv\"\n",
    "    fpath = os.path.join(\"simulation_data\", fname)\n",
    "    data_scrapped.to_csv(fpath, index=False)\n",
    "    \n",
    "    # Briefly summarise the scrapped vehicle data\n",
    "    print(f\"Scrapped vehicles: {len(scrapped_ids)}\")\n",
    "    for fuel in [\"DI\", \"PE\", \"EL\", \"HY\"]:\n",
    "        n = data_scrapped.loc[data_scrapped[\"original_fuel_type\"]==fuel, \"vehicle_id\"].nunique()\n",
    "        print(f\"{fuel}: {n}\")\n",
    "    \n",
    "    # Show the scrapped vehicles values counts by age\n",
    "    print(\"Scrapped vehicles by age:\")\n",
    "    print(data_scrapped.loc[data[\"last_test\"]==True, 'age_year'].round().value_counts().sort_index())\n",
    "    # And finally the mileage distribution by fuel type\n",
    "    print(\"Mileage by fuel type:\")\n",
    "    for fuel in [\"DI\", \"PE\", \"EL\", \"HY\"]:\n",
    "        print(f\"{fuel}: {data_scrapped.loc[(data_scrapped['original_fuel_type'] == fuel) & (data['last_test']==True), 'test_mileage'].mean()}\")\n",
    "\n",
    "    # Keep remaining vehicles for next round of predictions\n",
    "    data = data[~data[\"vehicle_id\"].isin(scrapped_ids)]\n",
    "    \n",
    "    # Break if iteration limit reached\n",
    "    if i == 30:\n",
    "        fname = \"simulation_data_end.csv\"\n",
    "        fpath = os.path.join(data_path, \"simulation_data\", fname)\n",
    "        data.to_csv(fpath, index=False)\n",
    "        break\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "# Aggregate all the scrapped data\n",
    "print(\"Aggregating scrapped data...\")\n",
    "df = []\n",
    "for i in range(0, i, 1):\n",
    "    print(f\"\\tReading in simulation_data_{i}.csv\")\n",
    "    fname = f\"simulation_data_{i}.csv\"\n",
    "    fpath = os.path.join(data_path, \"simulation_data\", fname)\n",
    "    df.append(pd.read_csv(fpath))\n",
    "\n",
    "print(\"Outputting aggregated scrapped data...\")\n",
    "df = pd.concat(df, ignore_index=True)\n",
    "fname = \"simulation_data.csv\"\n",
    "fpath = os.path.join(data_path, \"simulation_data\", fname)\n",
    "df.to_csv(fpath, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
